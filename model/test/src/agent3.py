import os
import asyncio
from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory  # âœ… LangChain ë©”ëª¨ë¦¬ ì¶”ê°€
from langchain_core.messages import HumanMessage, AIMessage
from dotenv import load_dotenv
from agents_system_prompts import assistant  # âœ… ê¸°ì¡´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ìœ ì§€




# âœ… 2. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… 3. LLM ëª¨ë¸ ì„¤ì •
HF_TOKEN = os.environ.get("HF_TOKEN")
HUGGINGFACE_REPO_ID = "meta-llama/Llama-3.3-70B-Instruct"

# âœ… 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ (Pydantic ëª¨ë¸ ì ìš©)
assistant_instance = assistant()
assistant_data = assistant_instance.model_dump()  # Pydantic ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë³€í™˜
system_prompt = f"{assistant_data['system_prompt']}\n\nRole: {assistant_data['role']}\nGoal: {assistant_data['goal']}"

# âœ… 5. LangChain ë©”ëª¨ë¦¬ (ëŒ€í™” ê¸°ë¡ ì €ì¥)
memory = ConversationBufferMemory(memory_key="messages", return_messages=True)


def load_llm():
    """LLM ë¡œë“œ (HuggingFace Inference API)"""
    try:
        return HuggingFaceEndpoint(
            repo_id=HUGGINGFACE_REPO_ID,
            task="text-generation",
            max_new_tokens=125,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.2,
            model_kwargs={
                "max_length": 216,
                "num_beams": 3,
            },
            huggingfacehub_api_token=HF_TOKEN,
        )
    except Exception as e:
        print(f"âŒ [LLM ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None


# âœ… 6. LLM ë¡œë“œ
llm = load_llm()


async def process_query(query: str, conversation_history: list):
    """ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ LLMì„ ì‹¤í–‰í•˜ê³ , ì¦‰ì‹œ ì§ˆë¬¸ì„ ì¶œë ¥"""
    if llm is None:
        return "âŒ LLMì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # âœ… ê¸°ì¡´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ì…ë ¥ ë©”ì‹œì§€ ìƒì„±
    messages = memory.load_memory_variables({}).get("messages", [])

    # âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì²« ë²ˆì§¸ ë©”ì‹œì§€ë¡œ ì¶”ê°€ (ì´ˆê¸°í™” ì‹œ)
    if not messages:
        messages.append(HumanMessage(content=system_prompt))

    # âœ… ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.append(HumanMessage(content=query))

    # âœ… ë©”ì‹œì§€ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ LLMì— ì „ë‹¬
    llm_input = "\n".join(
        [msg.content for msg in messages if isinstance(msg, HumanMessage)]
    )

    # âœ… ì˜ˆì™¸ì²˜ë¦¬: ë¹ˆ ì…ë ¥ ë°©ì§€
    if not llm_input.strip():
        return "âŒ ìœ íš¨í•œ ì…ë ¥ì´ ì—†ìŠµë‹ˆë‹¤."

    try:
        # âœ… LLM ì‹¤í–‰ (ë¬¸ìì—´ ì…ë ¥)
        response = await llm.ainvoke(llm_input)

        # âœ… **LLM ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ë°˜í™˜**
        return response

    except Exception as e:
        print(f"âŒ [DEBUG] LLM ì‹¤í–‰ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"âŒ [LLM ì‹¤í–‰ ì˜¤ë¥˜] {e}"


# âœ… 7. ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("âœ… ì‹¤í–‰ ì‹œì‘: Python Llama-3.3 AI ì±—ë´‡")
    while True:
        user_input = input("ğŸ’¬ ì§ˆë¬¸: ")
        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ğŸ”´ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # âœ… ì§ˆë¬¸ì„ ë„£ìœ¼ë©´ ë°”ë¡œ ì‘ë‹µ ì¶œë ¥
        answer = asyncio.run(process_query(user_input, []))
        print(f"ğŸ¤– AI: {answer}")
