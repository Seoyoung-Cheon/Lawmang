import os
import asyncio
from typing import List, Dict, Annotated, TypedDict
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEndpoint
from langgraph.graph.message import add_messages
from agents_system_prompts import assistant  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
from langchain_teddynote import logging
logging.langsmith("llamaproject")
# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()



# âœ… LangSmith í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±



# âœ… LLM ëª¨ë¸ ì„¤ì •
HF_TOKEN = os.getenv("HF_TOKEN")

HUGGINGFACE_REPO_ID = "meta-llama/Llama-3.3-70B-Instruct"

# âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
assistant_instance = assistant()
assistant_data = vars(assistant_instance)  # âœ… Pydantic ì œê±° í›„ vars() ì‚¬ìš©
system_prompt = f"{assistant_data['system_prompt']}\n\nRole: {assistant_data['role']}\nGoal: {assistant_data['goal']}"


# âœ… LLM ë¡œë“œ í•¨ìˆ˜
def load_llm():
    """LLM ë¡œë“œ (HuggingFace Inference API)"""
    try:
        return HuggingFaceEndpoint(
            repo_id=HUGGINGFACE_REPO_ID,
            task="text-generation",
            max_new_tokens=150,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.2,
            model_kwargs={"max_length": 300, "num_beams": 2},
            huggingfacehub_api_token=HF_TOKEN,
        )
    except Exception as e:
        print(f"âŒ [LLM ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None


# âœ… LLM ë¡œë“œ
llm = load_llm()


# âœ… LangGraph ìƒíƒœ ì •ì˜
class ChatState(TypedDict):
    messages: Annotated[list, add_messages]


def normalize_message(message):
    """ë©”ì‹œì§€ë¥¼ (role, message) íŠœí”Œë¡œ ë³€í™˜"""
    if isinstance(message, tuple) and len(message) == 2:
        return message
    elif isinstance(message, dict) and "content" in message:
        return (
            "assistant" if message.get("type") == "ai" else "user",
            message["content"],
        )
    return ("unknown", str(message))


def format_prompt(state: ChatState, user_query: str) -> str:
    """ì´ì „ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

    formatted_messages = []
    for msg in state["messages"][-5:]:  # ìµœê·¼ 5ê°œ ëŒ€í™”ë§Œ í¬í•¨
        if isinstance(msg, tuple) and len(msg) == 2:
            formatted_messages.append(f"{msg[0]}: {msg[1]}")
        elif isinstance(msg, str):  # âœ… ë‹¨ìˆœ ë¬¸ìì—´ì´ë©´ "Unknown" ì²˜ë¦¬
            formatted_messages.append(f"Unknown: {msg}")
        elif isinstance(msg, dict) and "content" in msg:  # âœ… dict í˜•íƒœ ë©”ì‹œì§€ ì²˜ë¦¬
            formatted_messages.append(f"AI: {msg['content']}")
        else:
            formatted_messages.append("Unknown Message Format")

    conversation_history = "\n".join(formatted_messages)

    return f"""{system_prompt}

    ì´ì „ ëŒ€í™” ê¸°ë¡:
    {conversation_history}

    ì‚¬ìš©ì: {user_query}
    AI:"""


def normalize_message(message):
    """ë©”ì‹œì§€ë¥¼ (role, message) íŠœí”Œë¡œ ë³€í™˜"""
    if isinstance(message, tuple) and len(message) == 2:
        return message
    elif isinstance(message, dict) and "content" in message:
        return (
            "assistant" if message.get("type") == "ai" else "user",
            message["content"],
        )
    return ("unknown", str(message))


async def chatbot(state: ChatState) -> Dict[str, ChatState]:
    if llm is None:
        return {"messages": state["messages"]}

    user_query = normalize_message(state["messages"][-1])[1]
    formatted_prompt = format_prompt(state, user_query)

    try:
        response = await llm.ainvoke(formatted_prompt)
        response_text = response if isinstance(response, str) else str(response)
        state["messages"].append(("user", user_query))
        state["messages"].append(("assistant", response_text))
        return {"messages": state["messages"]}
    except Exception as e:
        print(f"âŒ [LLM ì‹¤í–‰ ì˜¤ë¥˜] {e}")
        state["messages"].append(("assistant", f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"))
        return {"messages": state["messages"]}


# âœ… LangGraph ê·¸ë˜í”„ ìƒì„±
workflow = StateGraph(ChatState)

# âœ… ë…¸ë“œ ì¶”ê°€ (ë¹„ë™ê¸° í•¨ìˆ˜ ê·¸ëŒ€ë¡œ ì¶”ê°€ ê°€ëŠ¥)
workflow.add_node("chatbot", chatbot)

# âœ… ì‹œì‘ ë° ì¢…ë£Œ ì„¤ì •
workflow.set_entry_point("chatbot")
workflow.add_edge("chatbot", END)

# âœ… LangGraph ì‹¤í–‰ê¸° ì»´íŒŒì¼
graph_executor = workflow.compile()


# âœ… ë¹„ë™ê¸° ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
async def main():
    print("âœ… ì‹¤í–‰ ì‹œì‘: LangGraph ê¸°ë°˜ Llama-3.3 AI ì±—ë´‡")

    state: ChatState = {"messages": []}  # âœ… ChatState íƒ€ì… ìœ ì§€

    while True:
        user_input = input("ğŸ’¬ ì§ˆë¬¸: ")
        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ğŸ”´ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # âœ… LangGraph ì‹¤í–‰ (ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì ìš©) + state í¬í•¨
        async for event in graph_executor.astream(
            {"messages": state["messages"] + [("user", user_input)]}
        ):
            for value in event.values():
                state["messages"] = value["messages"]  # âœ… state ì—…ë°ì´íŠ¸
                print(f"ğŸ¤– AI: {value['messages'][-1][1]}")


# âœ… ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(main())