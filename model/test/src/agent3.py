import os
import asyncio
from typing import Dict, Annotated, TypedDict
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEndpoint
from langgraph.graph.message import add_messages
from agents_system_prompts import assistant
from langchain_teddynote import logging

# âœ… LangGraph ë¡œê¹… ì„¤ì •
logging.langsmith("llamaproject")

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… Hugging Face API ì„¤ì •
HF_TOKEN = os.getenv("HF_TOKEN")
HUGGINGFACE_REPO_ID = "meta-llama/Llama-3.3-70B-Instruct"

# âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ (role & goal ì œê±°)
assistant_instance = assistant()
SYSTEM_PROMPT = assistant_instance.system_prompt


# âœ… LLM ë¡œë“œ í•¨ìˆ˜
def load_llm():
    """LLM ë¡œë“œ (HuggingFace Inference API)"""
    return HuggingFaceEndpoint(
        repo_id=HUGGINGFACE_REPO_ID,
        task="text-generation",
        max_new_tokens=300,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.5,  # âœ… ë°˜ë³µ ìµœì†Œí™”
        model_kwargs={"max_length": 500, "num_beams": 2},  # âœ… ê³¼ë„í•œ ìƒì„± ë°©ì§€
        huggingfacehub_api_token=HF_TOKEN,
    )


# âœ… LLM ë¡œë“œ
llm = load_llm()


# âœ… LangGraph ìƒíƒœ ì •ì˜
class ChatState(TypedDict):
    messages: Annotated[list, add_messages]


# âœ… ë©”ì‹œì§€ ì •ê·œí™” í•¨ìˆ˜ (ì¤‘ë³µ ì²˜ë¦¬ ì œê±°)
def normalize_message(message):
    """ë©”ì‹œì§€ë¥¼ (role, message) íŠœí”Œë¡œ ë³€í™˜"""
    if isinstance(message, tuple) and len(message) == 2:
        return message
    elif isinstance(message, dict) and "content" in message:
        return (
            "assistant" if message.get("type") == "ai" else "user",
            message["content"],
        )
    return ("unknown", str(message))


# âœ… í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… í•¨ìˆ˜ (SYSTEM_PROMPT ì¤‘ë³µ ë°©ì§€)
def format_prompt(state: ChatState, user_query: str) -> str:
    return f"""{SYSTEM_PROMPT}
ì‚¬ìš©ì: {user_query}
previous_history:
AI:"""


# âœ… ì±—ë´‡ ë…¸ë“œ
async def chatbot(state: ChatState) -> Dict[str, ChatState]:
    """LangGraphì—ì„œ ì‹¤í–‰ë˜ëŠ” ì±—ë´‡ ë…¸ë“œ"""
    if llm is None:
        return {"messages": state["messages"]}

    user_query = normalize_message(state["messages"][-1])[1]
    formatted_prompt = format_prompt(state, user_query)

    try:
        response = await llm.ainvoke(formatted_prompt)
        response_text = response if isinstance(response, str) else str(response)

        # âœ… ìƒíƒœ ì—…ë°ì´íŠ¸
        state["messages"].extend([("user", user_query), ("assistant", response_text)])
        return {"messages": state["messages"]}

    except Exception as e:
        print(f"âŒ [LLM ì‹¤í–‰ ì˜¤ë¥˜] {e}")
        state["messages"].append(("assistant", f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"))
        return {"messages": state["messages"]}


# âœ… LangGraph ê·¸ë˜í”„ ìƒì„±
workflow = StateGraph(ChatState)
workflow.add_node("chatbot", chatbot)  # âœ… ì±—ë´‡ ë…¸ë“œ ì¶”ê°€
workflow.set_entry_point("chatbot")  # âœ… ì‹œì‘ ì§€ì  ì„¤ì •
workflow.add_edge("chatbot", END)  # âœ… ì¢…ë£Œ ì§€ì  ì„¤ì •
graph_executor = workflow.compile()  # âœ… ì‹¤í–‰ê¸° ì»´íŒŒì¼


# âœ… LangGraph ì‹¤í–‰ í•¨ìˆ˜
async def main():
    print("âœ… ì‹¤í–‰ ì‹œì‘: LangGraph ê¸°ë°˜ Llama-3.3 AI ì±—ë´‡")

    state: ChatState = {"messages": []}

    while True:
        user_input = input("ğŸ’¬ ì§ˆë¬¸: ")
        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ğŸ”´ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        async for event in graph_executor.astream(
            {"messages": state["messages"] + [("user", user_input)]}
        ):
            for value in event.values():
                state["messages"] = value["messages"]
                print(f"ğŸ¤– AI: {value['messages'][-1][1]}")


# âœ… ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(main())
