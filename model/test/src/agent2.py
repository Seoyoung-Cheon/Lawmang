import os
import asyncio
from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory  # âœ… LangChain ë©”ëª¨ë¦¬ ì¶”ê°€
from langchain_core.messages import HumanMessage, AIMessage
from dotenv import load_dotenv
from agents_system_prompts import assistant  # âœ… ê¸°ì¡´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ìœ ì§€

# âœ… 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… 2. LLM ëª¨ë¸ ì„¤ì •
HF_TOKEN = os.environ.get("HF_TOKEN")
HUGGINGFACE_REPO_ID = "meta-llama/Llama-3.3-70B-Instruct"

# âœ… 3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ (Pydantic ëª¨ë¸ ì ìš©)
assistant_instance = assistant()
assistant_data = assistant_instance.model_dump()  # Pydantic ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë³€í™˜
system_prompt = f"{assistant_data['system_prompt']}\n\nRole: {assistant_data['role']}\nGoal: {assistant_data['goal']}"

# âœ… 4. LangChain ë©”ëª¨ë¦¬ (ëŒ€í™” ê¸°ë¡ ì €ì¥)
memory = ConversationBufferMemory(memory_key="messages", return_messages=True)


def load_llm():
    """LLM ë¡œë“œ (HuggingFace Inference API)"""
    try:
        return HuggingFaceEndpoint(
            repo_id=HUGGINGFACE_REPO_ID,
            task="text-generation",
            max_new_tokens=250,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.2,
            model_kwargs={
                "max_length": 512,
                "num_beams": 3,
            },
            huggingfacehub_api_token=HF_TOKEN,
        )
    except Exception as e:
        print(f"âŒ [LLM ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None


# âœ… 5. LLM ë¡œë“œ
llm = load_llm()

# âœ… 6. LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€
prompt_template = PromptTemplate(
    input_variables=["system_prompt", "chat_history", "user_query"],
    template="""
    {system_prompt}
    
    ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    {chat_history}
    
    ì‚¬ìš©ìì˜ ìƒˆë¡œìš´ ì§ˆë¬¸: {user_query}

    ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¼ê´€ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
    """,
)


async def process_query(query: str):
    """ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ LLMì„ ì‹¤í–‰í•˜ê³ , ê°™ì€ ì§ˆë¬¸ì„ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡ ê°œì„ """
    if llm is None:
        return "âŒ LLMì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # âœ… ê¸°ì¡´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ì…ë ¥ ë©”ì‹œì§€ ìƒì„±
    messages = memory.load_memory_variables({}).get("messages", [])  # âœ… ë³€ê²½ë¨
    chat_history = "\n".join(
        [msg.content for msg in messages if isinstance(msg, HumanMessage)]
    )  # âœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¬¸ìì—´ë¡œ ë³€í™˜

    # âœ… íˆìŠ¤í† ë¦¬ë¥¼ ìš”ì•½í•˜ì—¬ ê°™ì€ ëŒ€í™” ë°˜ë³µì„ ë°©ì§€
    if len(messages) > 3:
        summary = f"ì§€ë‚œ ëŒ€í™” ìš”ì•½: {chat_history[-3:]}"
    else:
        summary = chat_history if chat_history else "ì´ì „ ëŒ€í™” ì—†ìŒ."

    # âœ… ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€ (ëŒ€í™” ê¸°ë¡ ì •ìƒ ë¡œë”© í™•ì¸)
    print(f"ğŸ” [DEBUG] chat_history type: {type(chat_history)}, value: {chat_history}")

    # âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ëŒ€í™” ë‚´ì—­ì„ í¬í•¨í•œ LLM ì…ë ¥ êµ¬ì„±
    try:
        formatted_prompt = prompt_template.format(
            system_prompt=system_prompt, chat_history=summary, user_query=query
        )
    except Exception as e:
        print(f"âŒ [DEBUG] í”„ë¡¬í”„íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return f"âŒ [í”„ë¡¬í”„íŠ¸ ìƒì„± ì˜¤ë¥˜] {e}"

    # âœ… ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€ (LLM ì…ë ¥ê°’ í™•ì¸)
    print(
        f"ğŸ” [DEBUG] llm.ainvoke() input type: {type(formatted_prompt)}, value: {formatted_prompt}"
    )

    # âœ… ì˜ˆì™¸ì²˜ë¦¬: ë¹ˆ ì…ë ¥ ë°©ì§€
    if not formatted_prompt.strip():
        return "âŒ ìœ íš¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨ (ì…ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.)"

    try:
        # âœ… LLM ì‹¤í–‰ (BaseMessages ì‚¬ìš©)
        response = await llm.ainvoke(formatted_prompt)

        # âœ… ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€ (LLM ì‘ë‹µ í™•ì¸)
        print(f"ğŸ” [DEBUG] LLM response type: {type(response)}, value: {response}")

        # âœ… LLM ì‘ë‹µì„ ë©”ëª¨ë¦¬ì— ì €ì¥ (ë°˜ë³µ ë°©ì§€) âœ… ì˜¬ë°”ë¥¸ ì €ì¥ ë°©ì‹ ì ìš©
        memory.save_context({"input": query}, {"output": response})

        # âœ… **LLM ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ë°˜í™˜**
        return response

    except Exception as e:
        print(f"âŒ [DEBUG] LLM ì‹¤í–‰ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ğŸ” [DEBUG] LLM input at error: {formatted_prompt}")
        return f"âŒ [LLM ì‹¤í–‰ ì˜¤ë¥˜] {e}"


# âœ… 7. ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("âœ… ì‹¤í–‰ ì‹œì‘: Python Llama-3.3 AI ì±—ë´‡")
    while True:
        user_input = input("ğŸ’¬ ì§ˆë¬¸: ")
        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ğŸ”´ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # âœ… ì§ˆë¬¸ì„ ë„£ìœ¼ë©´ ë°”ë¡œ ì‘ë‹µ ì¶œë ¥
        answer = asyncio.run(process_query(user_input))
        print(f"ğŸ¤– AI: {answer}")
