import os
import asyncio
from typing import List, Dict
from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, END, START  # âœ… START ì¶”ê°€
from dotenv import load_dotenv
from agents_system_prompts import assistant  # âœ… ê¸°ì¡´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìœ ì§€

# âœ… 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… 2. LLM ëª¨ë¸ ì„¤ì •
HF_TOKEN = os.getenv("HF_TOKEN")
HUGGINGFACE_REPO_ID = "meta-llama/Llama-3.3-70B-Instruct"

# âœ… 3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
assistant_instance = assistant()
assistant_data = vars(assistant_instance)  # âœ… Pydantic ì œê±° í›„ vars() ì‚¬ìš©
system_prompt = f"{assistant_data['system_prompt']}\n\nRole: {assistant_data['role']}\nGoal: {assistant_data['goal']}"


# âœ… 4. LLM ë¡œë“œ í•¨ìˆ˜
def load_llm():
    """LLM ë¡œë“œ (HuggingFace Inference API)"""
    try:
        return HuggingFaceEndpoint(
            repo_id=HUGGINGFACE_REPO_ID,
            task="text-generation",
            max_new_tokens=250,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.2,
            model_kwargs={"max_length": 512, "num_beams": 3},
            huggingfacehub_api_token=HF_TOKEN,
        )
    except Exception as e:
        print(f"âŒ [LLM ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None


# âœ… 5. LLM ë¡œë“œ
llm = load_llm()


# âœ… 6. LangGraph ìƒíƒœ ì§ì ‘ ì •ì˜ (Pydantic ë¯¸ì‚¬ìš©)
class LegalAIState:
    """LangGraph ìƒíƒœ ì •ì˜"""

    def __init__(self):
        self.chat_history: List[str] = []  # âœ… ì§ì ‘ ë¦¬ìŠ¤íŠ¸ ì •ì˜


# âœ… 7. LangGraph í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt_template = PromptTemplate(
    input_variables=["system_prompt", "chat_history", "user_query"],
    template="""{system_prompt}

    ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡:
    {chat_history}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {user_query}

    ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¼ê´€ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
    """,
)


# âœ… 8. LangGraph ë…¸ë“œ ì •ì˜ (LLM ì‹¤í–‰)
async def process_query(state: LegalAIState, query: str) -> Dict[str, object]:
    """LangGraph ë…¸ë“œ: LLM ì‹¤í–‰"""
    if llm is None:
        return {"state": state, "response": "âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    # âœ… ëŒ€í™” ê¸°ë¡ ë¡œë“œ
    chat_history = (
        "\n".join(state.chat_history[-3:]) if state.chat_history else "ì´ì „ ëŒ€í™” ì—†ìŒ."
    )

    # âœ… LangGraph ìƒíƒœ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
    formatted_prompt = prompt_template.format(
        system_prompt=system_prompt, chat_history=chat_history, user_query=query
    )

    try:
        response = await llm.ainvoke(formatted_prompt)

        # âœ… LangGraph ìƒíƒœ ì—…ë°ì´íŠ¸ (ëŒ€í™” ê¸°ë¡ ì¶”ê°€)
        state.chat_history.append(f"ì‚¬ìš©ì: {query}")
        state.chat_history.append(f"AI: {response}")

        return {
            "state": state,
            "response": response,
        }  # âœ… ë°˜ë“œì‹œ ìƒíƒœ(state)ì™€ ì‘ë‹µ(response)ë¥¼ ë¶„ë¦¬í•´ì„œ ë°˜í™˜

    except Exception as e:
        print(f"âŒ [LLM ì‹¤í–‰ ì˜¤ë¥˜] {e}")
        return {"state": state, "response": f"âŒ [LLM ì‹¤í–‰ ì˜¤ë¥˜] {e}"}  # âœ… ìƒíƒœ ìœ ì§€


# âœ… 9. LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± (0.3.1 ë²„ì „ ë°©ì‹)
workflow = StateGraph(LegalAIState)  # âœ… State ì§ì ‘ ì •ì˜
workflow.add_node("llm_response", process_query)  # âœ… ë…¸ë“œ ì¶”ê°€

# âœ… 10. LangGraph ì§„ì…ì  & ì¢…ë£Œ ì²˜ë¦¬
workflow.set_entry_point("llm_response")  # âœ… START ì„¤ì •
workflow.add_edge("llm_response", END)  # âœ… ì‹¤í–‰ í›„ ì¢…ë£Œ

# âœ… 11. LangGraph ì‹¤í–‰ê¸° ìƒì„±
graph_executor = workflow.compile()  # âœ… ì‹¤í–‰ ì „ì— ë°˜ë“œì‹œ ì»´íŒŒì¼ í•„ìš”

# âœ… 12. ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("âœ… ì‹¤í–‰ ì‹œì‘: Python Llama-3.3 AI ì±—ë´‡ (LangGraph 0.3.1)")

    state = LegalAIState()  # âœ… ì§ì ‘ ì •ì˜í•œ State í´ë˜ìŠ¤ ì‚¬ìš©

    while True:
        user_input = input("ğŸ’¬ ì§ˆë¬¸: ")
        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ğŸ”´ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # âœ… LangGraph ì‹¤í–‰ (ë¹„ë™ê¸° ë°©ì‹ ìœ ì§€)
        for output in graph_executor.stream({"state": state, "query": user_input}):
            print(f"ğŸ¤– AI: {output['response']}")  # âœ… ì‘ë‹µì„ ì¶œë ¥
