import os
import torch
from transformers import (
    BartForConditionalGeneration,
    AutoTokenizer,
    BertForSequenceClassification,
    AutoConfig,
)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from safetensors.torch import load_file
from dotenv import load_dotenv
from kobart import get_pytorch_kobart_model, get_kobart_tokenizer

from functools import lru_cache
from langchain_retriever import LangChainRetrieval


# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… FAISS ë²¡í„°DB ë¡œë“œ
DB_FAISS_PATH = "vectorstore/db_faiss"


def load_faiss():
    """FAISS ë¡œë“œ"""
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask"
        )
        return FAISS.load_local(
            DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True
        )
    except Exception as e:
        print(f"âŒ [FAISS ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None


langchain_retriever = LangChainRetrieval()


# âœ… BART íŒë¡€ ìš”ì•½ ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "./model/1_bart/checkpoint-26606"


# ì „ì—­ ìºì‹±
bart_model = None
bart_tokenizer = None


def load_bart():
    """KoBART ëª¨ë¸ ë¡œë“œ (ì „ì—­ ìºì‹± ì ìš©)"""
    global bart_model, bart_tokenizer
    if bart_model is None or bart_tokenizer is None:
        try:
            print("ğŸ” KoBART ëª¨ë¸ ë¡œë“œ ì¤‘...")
            bart_model = BartForConditionalGeneration.from_pretrained(
                get_pytorch_kobart_model()
            )
            bart_tokenizer = get_kobart_tokenizer()
            bart_tokenizer.pad_token_id = bart_tokenizer.eos_token_id
            bart_tokenizer.model_max_length = 1024

            # âœ… `model.safetensors`ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
            state_dict = load_file(os.path.join(MODEL_PATH, "model.safetensors"))
            bart_model.load_state_dict(state_dict, strict=False)

            # âœ… ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì „í™˜
            bart_model.eval()
            print("âœ… KoBART ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ [KoBART ë¡œë“œ ì˜¤ë¥˜] {e}")
            bart_model, bart_tokenizer = None, None
    return bart_tokenizer, bart_model


def summarize_case(text, tokenizer, model):
    """íŒë¡€ ìš”ì•½: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ê¸¸ì–´ì•¼ ìš”ì•½ì„ ìˆ˜í–‰í•˜ë„ë¡ í•¨"""
    try:
        char_length = len(text)
        word_count = len(text.split())
        print(
            f"ğŸ” [DEBUG] ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ììˆ˜): {char_length}, ë‹¨ì–´ ìˆ˜: {word_count}"
        )

        if word_count < 5 or char_length < 20:
            return "ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì§§ì•„ ìš”ì•½ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # âœ… í† í°í™” í›„ ì¸ì½”ë”©ëœ ê°’ í™•ì¸
        input_ids = tokenizer.encode(
            text, return_tensors="pt", max_length=1024, truncation=True
        )
        print(f"ğŸ” [DEBUG] BART input_ids: {input_ids}")

        # âœ… ëª¨ë¸ì˜ vocab_size ë²”ìœ„ ë‚´ë¡œ ê°’ ì œí•œ
        input_ids = torch.clamp(input_ids, min=0, max=model.config.vocab_size - 1)

        print("ğŸš€ [INFO] `generate()` ì‹¤í–‰ ì‹œì‘")

        summary_ids = model.generate(
            input_ids,
            max_length=150,  # âœ… ì‘ë‹µ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì§§ê²Œ ì„¤ì • (200 â†’ 150)
            min_length=120,  # âœ… ìµœì†Œí•œì˜ ì •ë³´ í¬í•¨ (80~120 ìœ ì§€)
            num_beams=3,  # âœ… beams ê°ì†Œë¡œ ì†ë„ ì¦ê°€ (8 â†’ 3)
            early_stopping=True,
            no_repeat_ngram_size=3,
            repetition_penalty=1.5,  # âœ… ë°˜ë³µ ìµœì†Œí™” (2.0 â†’ 1.5)
            length_penalty=0.8,  # âœ… ë” ì§§ì€ ìš”ì•½ ìƒì„± (1.0 â†’ 0.8)
        )  # ëª¨ë¸ 3 ì†ë„ë§Œ ë¹ ë¥´ê³  ë¶€ì •í™•
        print(f"ğŸ” [DEBUG] summary_ids: {summary_ids}")

        decoded = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        print(f"ğŸ” [DEBUG] ìš”ì•½ ê²°ê³¼: {decoded}")

        return decoded

    except Exception as e:
        print(f"âŒ [íŒë¡€ ìš”ì•½ ì˜¤ë¥˜] {e}")
        return "âŒ ìš”ì•½ ì‹¤íŒ¨"


# âœ… BERT íŒê²° ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ
JUDGMENT_MODEL_PATH = "./model/2_bert/20240222_best_bert.pth"


bert_model = None
bert_tokenizer = None


def load_bert():
    """BERT ëª¨ë¸ ë¡œë“œ (ì „ì—­ ìºì‹± ì ìš©)"""
    global bert_model, bert_tokenizer
    if bert_model is None or bert_tokenizer is None:
        try:
            print("ğŸ” BERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
            bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
            config = AutoConfig.from_pretrained("bert-base-uncased")
            config.num_labels = 3
            config.id2label = {0: "ë¬´ì£„", 1: "ìœ ì£„", 2: "ë¶ˆëª…í™•"}
            config.label2id = {"ë¬´ì£„": 0, "ìœ ì£„": 1, "ë¶ˆëª…í™•": 2}

            bert_model = BertForSequenceClassification.from_pretrained(
                "bert-base-uncased", config=config
            )
            state_dict = torch.load(JUDGMENT_MODEL_PATH, map_location="cpu")
            bert_model.load_state_dict(state_dict, strict=False)

            bert_model.eval()
            print("âœ… BERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ [BERT ë¡œë“œ ì˜¤ë¥˜] {e}")
            bert_model, bert_tokenizer = None, None
    return bert_tokenizer, bert_model


def predict_judgment(text, tokenizer, model):
    """íŒê²° ì˜ˆì¸¡"""
    try:
        inputs = tokenizer(
            text,
            return_tensors="pt",
            max_length=300,  # âœ… ë¶ˆí•„ìš”í•œ ì—°ì‚° ì¤„ì´ê¸° ìœ„í•´ 300ìœ¼ë¡œ ì„¤ì •
            truncation=True,
            padding="longest",  # âœ… ë¶ˆí•„ìš”í•œ íŒ¨ë”© ìµœì†Œí™”
        )  # 2ì°¨ ì¡°ì •

        inputs["attention_mask"] = torch.ones_like(inputs["input_ids"])

        with torch.no_grad():
            logits = model(**inputs).logits
            print(f"ğŸ” [DEBUG] BERT logits: {logits}")
            probabilities = torch.nn.functional.softmax(logits, dim=1)

        return probabilities.tolist()

    except Exception as e:
        print(f"âŒ [íŒê²° ì˜ˆì¸¡ ì˜¤ë¥˜] {e}")
        return "âŒ ì˜ˆì¸¡ ì‹¤íŒ¨"


@lru_cache(maxsize=1000)
def get_bart_model():
    return load_bart()


@lru_cache(maxsize=1000)
def get_bert_model():
    return load_bert()


def main():
    """CLI ê¸°ë°˜ ë²•ë¥  AI ì‹¤í–‰ (í„°ë¯¸ë„ ì…ë ¥)"""
    print("âœ… [ì‹œì‘] ë²•ë¥  AI ì‹¤í–‰")

    ## FAISS, BART, BERT ë¡œë“œ
    load_faiss()
    get_bart_model()
    load_bert()

    while True:
        user_query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ")
        if user_query.lower() == "exit":
            break

        # âœ… ê²€ìƒ‰ ì‹¤í–‰
        result = search(user_query)

        # âœ… ìµœì¢… ì¶œë ¥
        print("\nğŸ“Œ ê¸°ë³¸ ê²€ìƒ‰ ë‹µë³€:", result["search_result"])
        print("ğŸ“Œ íŒë¡€ ìš”ì•½:", result["summary"])
        print("\nğŸ¤– LLM ìµœì¢… ë‹µë³€:", result["final_answer"])


def search(query: str):
    """ğŸ” APIì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ìƒ‰ í•¨ìˆ˜"""
    print(f"ğŸ” [INFO] ê²€ìƒ‰ ì‹¤í–‰: {query}")

    # âœ… FAISS ê²€ìƒ‰
    response = "ë²•ë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    retrieved_text = ""

    db = load_faiss()
    if db:
        try:
            search_results = db.similarity_search(query, k=3)
            retrieved_text = "\n".join([doc.page_content for doc in search_results])
            response = retrieved_text
        except Exception as e:
            print(f"âŒ [FAISS ê²€ìƒ‰ ì˜¤ë¥˜] {e}")

    # âœ… íŒë¡€ ìš”ì•½
    summary = "BART ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ"
    summarizer_tokenizer, summarizer_model = get_bart_model()
    if summarizer_tokenizer and summarizer_model:
        summary = summarize_case(retrieved_text, summarizer_tokenizer, summarizer_model)

    # âœ… LangChainì„ í™œìš©í•œ ìµœì¢… ë‹µë³€ ìƒì„±
    final_answer = langchain_retriever.generate_legal_answer(query, summary)

    return {
        "search_result": response,
        "summary": summary,
        "final_answer": final_answer,
    }


if __name__ == "__main__":
    main()
