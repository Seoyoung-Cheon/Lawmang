import os
from transformers import (
    BartForConditionalGeneration,
    AutoTokenizer,
    BertForSequenceClassification,
)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv
from functools import lru_cache
from langchain_retriever import LangChainRetrieval
import numpy as np

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… FAISS ë²¡í„°DB ë¡œë“œ
DB_FAISS_PATH = "vectorstore/db_faiss"


def softmax(logits):
    """SciPy ì—†ì´ NumPyë§Œìœ¼ë¡œ softmax êµ¬í˜„"""
    exp_logits = np.exp(logits - np.max(logits))  # ì˜¤ë²„í”Œë¡œìš° ë°©ì§€
    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)

def load_faiss():
    """FAISS ë¡œë“œ"""
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask"
        )
        return FAISS.load_local(
            DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True
        )
    except Exception as e:
        print(f"âŒ [FAISS ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None


langchain_retriever = LangChainRetrieval()


# âœ… BART íŒë¡€ ìš”ì•½ ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "./model/1.íŒë¡€ìš”ì•½ëª¨ë¸/checkpoint-26606"


def load_bart():
    """KoBART ëª¨ë¸ ë¡œë“œ"""
    try:
        print("ğŸ” KoBART ëª¨ë¸ ë¡œë“œ ì¤‘...")

        # âœ… KoBART ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        model = BartForConditionalGeneration.from_pretrained(MODEL_PATH)
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        tokenizer.pad_token_id = tokenizer.eos_token_id

        # âœ… ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì „í™˜
        model.eval()
        print("âœ… KoBART ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return tokenizer, model

    except Exception as e:
        print(f"âŒ [KoBART ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None, None


def summarize_case(text, tokenizer, model):
    """íŒë¡€ ìš”ì•½"""
    try:
        if len(text.split()) < 5:
            return "ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì§§ì•„ ìš”ì•½ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        input_ids = tokenizer.encode(
            text, return_tensors="pt", max_length=1024, truncation=True
        )

        summary_ids = model.generate(
            input_ids,
            max_length=200,
            min_length=120,
            num_beams=6,
            early_stopping=True,
            no_repeat_ngram_size=3,
            repetition_penalty=1.8,
            length_penalty=1.0,
        )

        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    except Exception as e:
        print(f"âŒ [íŒë¡€ ìš”ì•½ ì˜¤ë¥˜] {e}")
        return "âŒ ìš”ì•½ ì‹¤íŒ¨"


# âœ… BERT íŒê²° ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ
JUDGMENT_MODEL_PATH = "./model/2.íŒê²°ì˜ˆì¸¡ëª¨ë¸/20240222_best_bert.pth"


def load_bert():
    """BERT íŒê²° ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ"""
    try:
        print("ğŸ” BERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(JUDGMENT_MODEL_PATH)
        model = BertForSequenceClassification.from_pretrained(JUDGMENT_MODEL_PATH)
        model.eval()
        print("âœ… BERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return tokenizer, model
    except Exception as e:
        print(f"âŒ [BERT ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None, None


# âœ… íŒê²° ì˜ˆì¸¡ í•¨ìˆ˜
def predict_judgment(text, tokenizer, model):
    try:
        inputs = tokenizer(
            text,
            return_tensors="np",  # âœ… NumPy ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½
            max_length=300,
            truncation=True,
            padding="longest",
        )
        logits = model(**inputs).logits  # âœ… `torch.no_grad()` ì œê±°
        logits = logits.detach().cpu().numpy()  # âœ… NumPy ë³€í™˜
        probabilities = softmax(logits)  # âœ… SciPy ì—†ì´ Softmax ê³„ì‚°
        return probabilities.tolist()
    except Exception as e:
        print(f"âŒ [íŒê²° ì˜ˆì¸¡ ì˜¤ë¥˜] {e}")
        return "âŒ ì˜ˆì¸¡ ì‹¤íŒ¨"


@lru_cache(maxsize=1000)
def get_bart_model():
    return load_bart()


@lru_cache(maxsize=1000)
def get_bert_model():
    return load_bert()


def main():
    try:
        print("âœ… [ì‹œì‘] ë²•ë¥  AI ì‹¤í–‰")

        ## FAISS, BART, BERT ë¡œë“œ
        db = load_faiss()
        summarizer_tokenizer, summarizer_model = get_bart_model()
        get_bert_model()  # âœ… BERT ëª¨ë¸ ë¡œë“œ ìˆ˜ì •

        while True:
            user_query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ")
            if user_query.lower() == "exit":
                break

            response = "ë²•ë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            retrieved_text = ""

            if db:
                try:
                    search_results = db.similarity_search(user_query, k=3)
                    retrieved_text = "\n".join(
                        [doc.page_content for doc in search_results]
                    )
                    response = retrieved_text
                except Exception as e:
                    print(f"âŒ [FAISS ê²€ìƒ‰ ì˜¤ë¥˜] {e}")

            summary = "BART ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ"
            if summarizer_tokenizer and summarizer_model:
                summary = summarize_case(
                    retrieved_text, summarizer_tokenizer, summarizer_model
                )

            final_answer = langchain_retriever.generate_legal_answer(
                user_query, summary
            )

            print("\nğŸ“Œ ê¸°ë³¸ ê²€ìƒ‰ ë‹µë³€:", response)
            print("ğŸ“Œ íŒë¡€ ìš”ì•½:", summary)
            print("\nğŸ¤– LLM ìµœì¢… ë‹µë³€:", final_answer)

    except Exception as e:
        print(f"\nâŒ [main() ì‹¤í–‰ ì˜¤ë¥˜] {e}")


if __name__ == "__main__":
    main()
