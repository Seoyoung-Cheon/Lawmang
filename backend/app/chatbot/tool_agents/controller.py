from app.chatbot.tool_agents.qualifier import run_consultation_qualifier
from app.chatbot.tool_agents.planner import (
    generate_response_template,
    run_response_strategy_with_limit,
)
from app.chatbot.tool_agents.precedent import LegalPrecedentRetrievalAgent
from app.chatbot.tool_agents.executor.normalanswer import run_final_answer_generation
from app.chatbot.tool_agents.tools import async_search_consultation
from app.chatbot.memory.global_cache import (
    get_cached_result,
    update_cached_result,
    cache_intermediate_result,
)
from typing import List


async def run_full_consultation(
    user_query: str,
    search_keywords: List[str],
    model: str = "gpt-4",
) -> dict:
    session_id = user_query[:20]
    cached = get_cached_result(session_id)
    print("âœ… [user_query í™•ì¸]:", user_query)
    print("âœ… [search_keywords í™•ì¸]:", search_keywords)

    if not isinstance(model, str):
        raise TypeError(
            f"âŒ model ì¸ìëŠ” ë¬¸ìì—´(str)ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ íƒ€ì…: {type(model)}, ê°’: {model}"
        )


    # 1ï¸âƒ£ Qualifier ì‹¤í–‰
    consultation_results, _, _ = await async_search_consultation(search_keywords)
    best_case = await run_consultation_qualifier(user_query, consultation_results)
    if best_case.get("status") in ["no_match", "irrelevant", "parse_error"]:
        return {"status": "fail_qualifier", "error": "ì ì ˆí•œ ìƒë‹´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    title = best_case["title"]
    question = best_case["question"]
    answer = best_case["answer"]

    # 2ï¸âƒ£ Template ìƒì„±
    if not cached or "template" not in cached:
        template = await generate_response_template(title, question, answer, user_query)
        cache_intermediate_result(
            session_id,
            {
                "template": template,
                "strategy": None,
                "precedent": None,
                "escalated_once": False,  # ìºì‹œì— ìƒíƒœë„ í•¨ê»˜ ì €ì¥
            },
        )
    else:
        template = cached["template"]

    # 3ï¸âƒ£ Strategy ìƒì„±
    if not cached or not cached.get("strategy"):
        strategy = await run_response_strategy_with_limit(
            template["explanation"],
            user_query,
            template.get("hyperlinks", []),
        )
        update_cached_result(session_id, "strategy", strategy)
    else:
        strategy = cached["strategy"]

    # 4ï¸âƒ£ Precedent ê²€ìƒ‰
    if not cached or not cached.get("precedent"):
        precedent_agent = LegalPrecedentRetrievalAgent()
        precedent = await precedent_agent.run(
            categories=[title], titles=[title], user_input_keywords=search_keywords
        )
        update_cached_result(session_id, "precedent", precedent)
    else:
        precedent = cached["precedent"]

    print("\nğŸ§ª [TEMPLATE ìƒì„± ì™„ë£Œ]:", template.get("summary", "ìš”ì•½ ì—†ìŒ"))
    print(
        "ğŸ§ª [STRATEGY ìƒì„± ì™„ë£Œ]:", strategy.get("final_strategy_summary", "ìš”ì•½ ì—†ìŒ")
    )
    print("ğŸ§ª [PRECEDENT ìƒì„± ì™„ë£Œ]:", precedent.get("summary", "ìš”ì•½ ì—†ìŒ"))

    # 5ï¸âƒ£ YES 3íšŒ ë¯¸ë§Œì´ë©´ ìµœì¢… ì‘ë‹µ ìƒëµ
    if not cached.get("escalated_once", False):
        print("â¸ï¸ [LLM2 ì‹¤í–‰ ë³´ë¥˜] ì•„ì§ YES ì¹´ìš´íŠ¸ 3 ë¯¸ë§Œì´ë¯€ë¡œ ìµœì¢… ì‘ë‹µ ìƒëµ")
        return {
            "user_query": user_query,
            "template": template,
            "strategy": strategy,
            "precedent": precedent,
            "final_answer": None,
            "status": "intermediate_ready",
        }

    # 6ï¸âƒ£ Final Answer ìƒì„±
    final_answer = run_final_answer_generation(
        template=template,
        strategy=strategy,
        precedent=precedent,
        user_query=user_query,
        model=model,
    )

    return {
        "user_query": user_query,
        "template": template,
        "strategy": strategy,
        "precedent": precedent,
        "final_answer": final_answer,
        "status": "ok",
    }
