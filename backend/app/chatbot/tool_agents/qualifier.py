# qualifier.py
import json
import asyncio
import openai
from typing import List, Dict
from tools import async_search_consultation

openai.api_key = "YOUR_OPENAI_API_KEY"


def build_relevance_prompt(user_query: str, consultation_results: List[Dict]) -> str:
    """
    GPTì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸: ìƒë‹´ ëª©ë¡ì´ user_queryì™€ ê´€ë ¨ ìˆëŠ”ì§€ íŒë‹¨
    """
    formatted = "\n\n".join(
        [
            f"{idx + 1}. ì œëª©: {item['title']}\nì§ˆë¬¸: {item['question']}"
            for idx, item in enumerate(consultation_results)
        ]
    )

    return f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
\"{user_query}\"

ì•„ë˜ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆëŠ” ê¸°ì¡´ ìƒë‹´ë“¤ì…ë‹ˆë‹¤.

ê° í•­ëª©ì€ 'ì œëª©', 'ì§ˆë¬¸'ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  
â†’ ë§Œì•½ ì•„ë˜ ìƒë‹´ë“¤ì´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ **ì£¼ì œì ìœ¼ë¡œ ì™„ì „íˆ ë¬´ê´€**í•˜ë‹¤ë©´, "irrelevant"ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.  
â†’ ì¼ë¶€ë¼ë„ ê´€ë ¨ì´ ìˆë‹¤ë©´, "relevant"ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.

===== ìƒë‹´ ëª©ë¡ =====
{formatted}
""".strip()


def check_relevance_to_consultations(
    user_query: str,
    consultation_results: List[Dict],
    model: str = "gpt-3.5-turbo",
) -> bool:
    """
    GPTê°€ íŒë‹¨í•œ ìœ ì € ì§ˆë¬¸ì˜ ê´€ë ¨ì„± (ìƒë‹´ ë¦¬ìŠ¤íŠ¸ì™€ ë¬´ê´€í•œì§€ ì—¬ë¶€ íŒë‹¨)
    """
    if not consultation_results:
        return False

    prompt = build_relevance_prompt(user_query, consultation_results)

    response = openai.ChatCompletion.create(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì§ˆë¬¸ì˜ ì£¼ì œ ê´€ë ¨ì„±ì„ íŒë³„í•˜ëŠ” AIì…ë‹ˆë‹¤.",
            },
            {"role": "user", "content": prompt},
        ],
        temperature=0.0,
    )

    result_text = response.choices[0].message["content"].strip().lower()
    print("âœ… [ê´€ë ¨ì„± íŒë‹¨ ê²°ê³¼]:", result_text)
    return result_text == "relevant"

def build_choose_one_prompt(user_query: str, consultation_results: List[Dict]) -> str:
    """
    GPTì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸: ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê´€ë ¨ëœ ìƒë‹´ í•˜ë‚˜ ì„ íƒ
    """
    formatted = "\n\n".join(
        [
            f"{idx + 1}. ì œëª©: {item['title']}\nì§ˆë¬¸: {item['question']}\në‹µë³€: {item['answer']}"
            for idx, item in enumerate(consultation_results)
        ]
    )

    return f"""
    ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    \"{user_query}\"

    ì•„ë˜ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆëŠ” ìƒë‹´ ë°ì´í„° ëª©ë¡ì…ë‹ˆë‹¤.  
    ê° í•­ëª©ì€ ì œëª©, ì§ˆë¬¸, ë‹µë³€ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

    â›” **ì£¼ì˜:** ì§ˆë¬¸/ë‹µë³€ì´ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì£¼ì œì™€ ì–´ê¸‹ë‚œ í•­ëª©ì€ ì„ íƒí•˜ì§€ ë§ˆì„¸ìš”.  
    âœ… ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•˜ê²Œ ë‹µí•  ìˆ˜ ìˆëŠ” ìƒë‹´ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.

    â†’ **ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSON ë°°ì—´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.**  
    ì„ íƒí•  í•­ëª©ì˜ ë²ˆí˜¸ë§Œ ë°˜í™˜í•´ì•¼ í•˜ë©°, ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì œê³µí•˜ì„¸ìš”.  

    ì˜ˆì‹œ:  
    [2]

    â†’ ê´€ë ¨ëœ í•­ëª©ì´ **ì „í˜€ ì—†ë‹¤ë©´**, ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”.  

    ì˜ˆì‹œ:  
    []

===== ìƒë‹´ ëª©ë¡ =====
{formatted}
""".strip()


def choose_best_consultation(
    user_query: str,
    consultation_results: List[Dict],
    model: str = "gpt-3.5-turbo",
) -> Dict:
    """
    GPTë¥¼ ì´ìš©í•´ ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•œ ìƒë‹´ í•˜ë‚˜ë¥¼ ì„ íƒ
    """
    if not consultation_results:
        return {"error": "ğŸ” ê´€ë ¨ëœ ìƒë‹´ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", "status": "no_result"}

    prompt = build_choose_one_prompt(user_query, consultation_results)

    response = openai.ChatCompletion.create(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
            },
            {"role": "user", "content": prompt},
        ],
        temperature=0.1,
    )

    result_text = response.choices[0].message["content"]
    print("âœ… [Best ìƒë‹´ ì„ íƒ ê²°ê³¼]:", result_text)

    if result_text.strip() in ["[]", "[0]"]:
        return {"error": "ğŸ™ ê´€ë ¨ëœ ìƒë‹´ì´ ì—†ìŠµë‹ˆë‹¤.", "status": "irrelevant"}

    try:
        selected = json.loads(result_text)
        selected_index = int(selected[0]) if isinstance(selected, list) else None

        if selected_index and 0 < selected_index <= len(consultation_results):
            return consultation_results[selected_index - 1]
        else:
            return {
                "error": "â— ì„ íƒëœ ì¸ë±ìŠ¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                "status": "invalid_index",
            }

    except Exception as e:
        print("âŒ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:", e)
        return {"error": "â— GPT ì‘ë‹µì„ ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "status": "parse_error"}
    
    
async def run_consultation_qualifier(
    user_query: str,
    model: str = "gpt-3.5-turbo",
) -> Dict:
    """
    âœ… ëª¨ë“  ì¿¼ë¦¬ëŠ” async_search_consultationì„ ê±°ì³ ìœ ì‚¬ ìƒë‹´ ê²€ìƒ‰ë¨.
    ì´í›„ LLMìœ¼ë¡œ ê´€ë ¨ì„± ì²´í¬ + ìµœì  ìƒë‹´ ì„ íƒê¹Œì§€ ìˆ˜í–‰ë¨.
    """
    # ğŸ” 1) FAISS + SQL + ì „ì²˜ë¦¬ í¬í•¨ ê²€ìƒ‰
    consultation_results, _, _ = await async_search_consultation([user_query])

    if not consultation_results:
        return {
            "error": "ğŸ” ê´€ë ¨ëœ ìƒë‹´ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
            "status": "no_result",
        }

    # âœ… 2) LLM ê´€ë ¨ì„± íŒë‹¨
    is_relevant = check_relevance_to_consultations(
        user_query, consultation_results, model=model
    )
    if not is_relevant:
        return {
            "error": "ğŸ™ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìƒë‹´ì´ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‘ì„±í•´ë³´ì„¸ìš”.",
            "status": "no_match",
        }

    # âœ… 3) ê°€ì¥ ì ì ˆí•œ ìƒë‹´ ì„ íƒ
    return choose_best_consultation(user_query, consultation_results, model=model)