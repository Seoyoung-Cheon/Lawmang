import os
import json
from typing import List, Dict
from langchain_openai import ChatOpenAI
from app.chatbot.tool_agents.tools import async_search_consultation

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")


def build_relevance_prompt(user_query: str, consultation_results: List[Dict]) -> str:
    formatted = "\n\n".join(
        [
            f"{idx + 1}. ì œëª©: {item['title']}\nì§ˆë¬¸: {item['question']}"
            for idx, item in enumerate(consultation_results)
        ]
    )
    return f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
\"{user_query}\"

ì•„ë˜ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆëŠ” ê¸°ì¡´ ìƒë‹´ë“¤ì…ë‹ˆë‹¤.

ê° í•­ëª©ì€ 'ì œëª©', 'ì§ˆë¬¸'ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  
â†’ ë§Œì•½ ì•„ë˜ ìƒë‹´ë“¤ì´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ **ì£¼ì œì ìœ¼ë¡œ ì™„ì „íˆ ë¬´ê´€**í•˜ë‹¤ë©´, "irrelevant"ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.  
â†’ ì¼ë¶€ë¼ë„ ê´€ë ¨ì´ ìˆë‹¤ë©´, "relevant"ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.

===== ìƒë‹´ ëª©ë¡ =====
{formatted}
""".strip()


def check_relevance_to_consultations(
    user_query: str,
    consultation_results: List[Dict],
    model: str = "gpt-3.5-turbo",
) -> bool:
    if not consultation_results:
        return False
    print("âœ… model ì¸ì í™•ì¸:", model, type(model))  # â† ì´ê±° ì°ì–´ë³´ë©´ íŠœí”Œì¸ì§€ ì•Œ ìˆ˜ ìˆìŒ

    prompt = build_relevance_prompt(user_query, consultation_results)

    llm = ChatOpenAI(
        model=model,
        api_key=OPENAI_API_KEY,
        temperature=0.0,
        streaming=False
    )

    messages = [
        {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì§ˆë¬¸ì˜ ì£¼ì œ ê´€ë ¨ì„±ì„ íŒë³„í•˜ëŠ” AIì…ë‹ˆë‹¤.",
        },
        {"role": "user", "content": prompt},
    ]

    response = llm.invoke(messages)
    result_text = response.content.strip().lower()
    print("âœ… [ê´€ë ¨ì„± íŒë‹¨ ê²°ê³¼]:", result_text)
    return result_text == "relevant"


def build_choose_one_prompt(user_query: str, consultation_results: List[Dict]) -> str:
    formatted = "\n\n".join(
        [
            f"{idx + 1}. ì œëª©: {item['title']}\nì§ˆë¬¸: {item['question']}\në‹µë³€: {item['answer']}"
            for idx, item in enumerate(consultation_results)
        ]
    )
    return f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
\"{user_query}\"

ì•„ë˜ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆëŠ” ìƒë‹´ ë°ì´í„° ëª©ë¡ì…ë‹ˆë‹¤.  
ê° í•­ëª©ì€ ì œëª©, ì§ˆë¬¸, ë‹µë³€ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

â›” **ì£¼ì˜:** ì§ˆë¬¸/ë‹µë³€ì´ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì£¼ì œì™€ ì–´ê¸‹ë‚œ í•­ëª©ì€ ì„ íƒí•˜ì§€ ë§ˆì„¸ìš”.  
âœ… ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•˜ê²Œ ë‹µí•  ìˆ˜ ìˆëŠ” ìƒë‹´ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.

â†’ **ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSON ë°°ì—´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.**  
ì„ íƒí•  í•­ëª©ì˜ ë²ˆí˜¸ë§Œ ë°˜í™˜í•´ì•¼ í•˜ë©°, ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì œê³µí•˜ì„¸ìš”.  

ì˜ˆì‹œ:  
[2]

â†’ ê´€ë ¨ëœ í•­ëª©ì´ **ì „í˜€ ì—†ë‹¤ë©´**, ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”.  

ì˜ˆì‹œ:  
[]

===== ìƒë‹´ ëª©ë¡ =====
{formatted}
""".strip()


def choose_best_consultation(
    user_query: str,
    consultation_results: List[Dict],
    model: str = "gpt-3.5-turbo",
) -> Dict:
    if not consultation_results:
        return {"error": "ğŸ” ê´€ë ¨ëœ ìƒë‹´ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", "status": "no_result"}

    prompt = build_choose_one_prompt(user_query, consultation_results)

    llm = ChatOpenAI(
        model=model,
        api_key=OPENAI_API_KEY,
        temperature=0.1,
        streaming=False
    )

    messages = [
        {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        },
        {"role": "user", "content": prompt},
    ]

    response = llm.invoke(messages)
    result_text = response.content
    print("âœ… [Best ìƒë‹´ ì„ íƒ ê²°ê³¼]:", result_text)

    if result_text.strip() in ["[]", "[0]"]:
        return {"error": "ğŸ™ ê´€ë ¨ëœ ìƒë‹´ì´ ì—†ìŠµë‹ˆë‹¤.", "status": "irrelevant"}

    try:
        selected = json.loads(result_text)
        selected_index = int(selected[0]) if isinstance(selected, list) else None

        if selected_index and 0 < selected_index <= len(consultation_results):
            return consultation_results[selected_index - 1]
        else:
            return {
                "error": "â— ì„ íƒëœ ì¸ë±ìŠ¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                "status": "invalid_index",
            }
    except Exception as e:
        print("âŒ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:", e)
        return {"error": "â— GPT ì‘ë‹µì„ ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "status": "parse_error"}


async def run_consultation_qualifier(
    user_query: str,
    consultation_results: List[Dict],
    model: str = "gpt-3.5-turbo",
) -> Dict:
    if not consultation_results:
        return {
            "error": "ğŸ” ê´€ë ¨ëœ ìƒë‹´ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
            "status": "no_result",
        }

    is_relevant = check_relevance_to_consultations(
        user_query, consultation_results, model=model
    )
    if not is_relevant:
        return {
            "error": "ğŸ™ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìƒë‹´ì´ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‘ì„±í•´ë³´ì„¸ìš”.",
            "status": "no_match",
        }

    return choose_best_consultation(user_query, consultation_results, model=model)
