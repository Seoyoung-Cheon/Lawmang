import os
import json
import sys
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from app.chatbot.tool_agents.tools import LawGoKRTavilySearch
from app.chatbot.tool_agents.utils.utils import (
    insert_hyperlinks_into_text,
    extract_json_from_text
)
from langchain.memory import ConversationBufferMemory
from langchain_teddynote import logging

logging.langsmith("llamaproject")

# âœ… LangChain ChatOpenAI
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

memory = ConversationBufferMemory(
    memory_key="chat_history", max_token_limit=1000, return_messages=True
)


def build_final_answer_prompt(
    template: dict, strategy: dict, precedent: dict, user_query: str
) -> str:
    precedent_summary = precedent.get("summary", "")
    precedent_link = precedent.get("casenote_url", "")
    precedent_meta = f"{precedent.get('court', '')} / {precedent.get('j_date', '')} / {precedent.get('title', '')}"

    summary_with_links = insert_hyperlinks_into_text(
        template["summary"], template.get("hyperlinks", [])
    )
    explanation_with_links = insert_hyperlinks_into_text(
        template["explanation"], template.get("hyperlinks", [])
    )
    hyperlinks_text = "\n".join(
        [f"- {link['label']}: {link['url']}" for link in template.get("hyperlinks", [])]
    )
    strategy_decision_tree = "\n".join(strategy.get("decision_tree", []))

    chat_history = memory.load_memory_variables({}).get("chat_history", "")

    prompt = f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ì„ ìƒì„±í•˜ëŠ” ê³ ê¸‰ AIì…ë‹ˆë‹¤.

[ëŒ€í™” íˆìŠ¤í† ë¦¬]
{chat_history}

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_query}

[ìš”ì•½]
{summary_with_links}

[ì„¤ëª…]
{explanation_with_links}

[ì°¸ê³  ì§ˆë¬¸]
{template["ref_question"]}

[í•˜ì´í¼ë§í¬]
{hyperlinks_text}

[ì „ëµ ìš”ì•½]
{strategy.get("final_strategy_summary", "")}

[ì‘ë‹µ êµ¬ì„± ì „ëµ]
- ë§íˆ¬: {strategy.get("tone", "")}
- íë¦„: {strategy.get("structure", "")}
- ì¡°ê±´ íë¦„ë„:
{strategy_decision_tree}

[ì¶”ì²œ ë§í¬]
{json.dumps(strategy.get("recommended_links", []), ensure_ascii=False)}

[ì¶”ê°€ëœ íŒë¡€ ìš”ì•½]
- {precedent_summary}
- ë§í¬: {precedent_link}
- ì •ë³´: {precedent_meta}

ğŸ’¡ ìœ„ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬, ì‚¬ìš©ìê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë²•ë¥  ìƒë‹´ì„ ìƒì„±í•˜ì„¸ìš”.
"""
    return prompt.strip()


def run_final_answer_generation(
    template: dict,
    strategy: dict,
    precedent: dict,
    user_query: str,
    model: str = "gpt-4",
) -> str:
    final_prompt = build_final_answer_prompt(template, strategy, precedent, user_query)

    print("\nğŸ¤– AI ë‹µë³€:")
    final_answer = ""

    # âœ… LangChain ChatOpenAI (Streaming)
    llm = ChatOpenAI(
        model=model,
        api_key=OPENAI_API_KEY,
        temperature=0.4,
        streaming=True
    )

    messages = [
        SystemMessage(
            content="ë‹¹ì‹ ì€ ê³ ê¸‰ ë²•ë¥  ì‘ë‹µì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì‹ ë¢°ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ìƒë‹´ì„ ìƒì„±í•˜ì„¸ìš”."
        ),
        HumanMessage(content=final_prompt),
    ]

    # âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
    for chunk in llm.stream(messages):
        if hasattr(chunk, "content") and chunk.content:
            sys.stdout.write(chunk.content)
            sys.stdout.flush()
            final_answer += chunk.content

    print("\nâœ… [ìµœì¢… LLM ì‘ë‹µ ì™„ë£Œ]")

    # âœ… ë©”ëª¨ë¦¬ì— ì €ì¥
    memory.save_context(
        {"user_query": user_query}, {"response": precedent.get("summary", "")}
    )

    return final_answer


async def evaluate_final_answer_with_tavily(
    final_answer: str, user_query: str, model="gpt-4"
) -> dict:
    search_tool = LawGoKRTavilySearch(max_results=3)
    tavily_results = search_tool.run(user_query)

    tavily_snippets = []
    for result in tavily_results:
        content = result.get("content") or result.get("snippet") or result.get("text")
        if content:
            tavily_snippets.append(content.strip())

    combined_snippets = "\n\n".join(
        [f"[ìš”ì•½ {i + 1}]\n{snippet}" for i, snippet in enumerate(tavily_snippets)]
    )

    prompt = f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì‘ë‹µì„ í‰ê°€í•˜ëŠ” AIì…ë‹ˆë‹¤.

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_query}

[GPT ìµœì¢… ì‘ë‹µ]
{final_answer}

[Tavily ìš”ì•½ ê²°ê³¼ë“¤]
{combined_snippets}

--- ì‘ì—… ì§€ì‹œ ---
1. GPT ì‘ë‹µì´ Tavily ìš”ì•½ê³¼ ë¹„êµí–ˆì„ ë•Œ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶€ì‹¤í•˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”.
2. ë³´ì™„ì´ í•„ìš”í•˜ë‹¤ë©´ "needs_fix": true ë˜ëŠ” false ë¡œ ì‘ì„±í•˜ì„¸ìš”.

âš ï¸ ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ **JSON** í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆì‹œ:
{{
  "needs_fix": true,
  "reason": "ì‘ë‹µì— í•µì‹¬ ë²•ë ¹ì´ ë¹ ì ¸ ìˆìŠµë‹ˆë‹¤.",
  "fix_suggestion": "êµí†µì‚¬ê³ ì²˜ë¦¬íŠ¹ë¡€ë²• ì œ3ì¡°ë¥¼ í¬í•¨í•˜ì—¬ ì„¤ëª…ì„ ë³´ì™„í•˜ì„¸ìš”."
}}
"""
    llm = ChatOpenAI(
        model=model,
        api_key=OPENAI_API_KEY,
        temperature=0.2,
        streaming=False
    )

    messages = [
        SystemMessage(content="ë„ˆëŠ” ë²•ë¥  ì‘ë‹µ ê²€í†  ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤."),
        HumanMessage(content=prompt),
    ]

    response = llm.invoke(messages)
    result_text = response.content
    print("âœ… [ìµœì¢… ì‘ë‹µ í‰ê°€ ê²°ê³¼]:", result_text)

    try:
        extracted_json = extract_json_from_text(result_text)
        if not extracted_json:
            raise ValueError("ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        return json.loads(extracted_json)

    except Exception as e:
        print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨:", e)
        return {
            "needs_fix": False,
            "reason": "GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨",
            "fix_suggestion": "",
        }
