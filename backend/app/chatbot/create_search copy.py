import os
import sys
import torch
import asyncio
import numpy as np
from transformers import (
    BartForConditionalGeneration,
)
from sklearn.metrics.pairwise import cosine_similarity
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from safetensors.torch import load_file
from dotenv import load_dotenv
from kobart import get_pytorch_kobart_model, get_kobart_tokenizer
from kiwipiepy import Kiwi
from backend.app.chatbot.legal_response_generator import LangChainRetrieval
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
# from app.chatbot.tool_agents.tools import search_precedents
from app.chatbot.tool_agents.tools import async_search_consultation
from app.chatbot.tool_agents.tools import async_search_precedent
from app.chatbot.tool_agents.tools import search_tavily_for_precedents

# âœ… Kiwi ê°ì²´ ì „ì—­ ìºì‹±
kiwi = Kiwi()
# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
# âœ… FAISS ë²¡í„°DB ë¡œë“œ
DB_FAISS_PATH = "./app/chatbot/faiss"


def load_faiss():
    """FAISS ë¡œë“œ"""
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask"
        )
        return FAISS.load_local(
            DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True
        )
    except Exception as e:
        print(f"âŒ [FAISS ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None


def jaccard_similarity(set1, set2):
    """Jaccard ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ë¹„êµ"""
    intersection = set1.intersection(set2)
    union = set1.union(set2)
    return len(intersection) / len(union) if len(union) > 0 else 0


def extract_keywords(text, top_k=5):
    """í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•´ ëª…ì‚¬(NNG, NNP) ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    tokens = kiwi.tokenize(text)
    nouns = [token.form for token in tokens if token.tag in ("NNG", "NNP")]

    keyword_freq = {word: nouns.count(word) for word in set(nouns)}
    sorted_keywords = sorted(keyword_freq, key=keyword_freq.get, reverse=True)[:top_k]

    print(f"âœ… [í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼] {sorted_keywords}")
    return sorted_keywords


def filter_keywords_with_jaccard(user_keywords, faiss_keywords, threshold=0.15):
    """ìì¹´ë“œ ìœ ì‚¬ë„ë¥¼ í™œìš©í•˜ì—¬ FAISS í‚¤ì›Œë“œë¥¼ í•„í„°ë§ (ìœ ì € í‚¤ì›Œë“œ ìœ ì§€)"""
    filtered_keywords = set(user_keywords)  # âœ… ìœ ì € ì…ë ¥ í‚¤ì›Œë“œë¥¼ ë¬´ì¡°ê±´ í¬í•¨

    for faiss_word in faiss_keywords:
        max_sim = max(
            jaccard_similarity(set(faiss_word), set(user_word))
            for user_word in user_keywords
        )

        if max_sim >= threshold:  # âœ… ì¼ì • ìœ ì‚¬ë„ ì´ìƒì¸ FAISS í‚¤ì›Œë“œë§Œ ì¶”ê°€
            filtered_keywords.add(faiss_word)

    return list(filtered_keywords)  # âœ… ìµœì¢… í‚¤ì›Œë“œ ë°˜í™˜ (ìœ ì € í‚¤ì›Œë“œ í¬í•¨)

def filter_consultation_keywords(user_keywords, consultation_keywords, threshold=0.15):
    """ğŸ” Jaccard ìœ ì‚¬ë„ë¥¼ í™œìš©í•˜ì—¬ ë²•ë¥  ìƒë‹´ í‚¤ì›Œë“œë¥¼ í•„í„°ë§"""
    filtered_keywords = set(user_keywords)  # âœ… ìœ ì € ì…ë ¥ í‚¤ì›Œë“œëŠ” ë¬´ì¡°ê±´ í¬í•¨

    for cons_word in consultation_keywords:
        cons_word_set = set(cons_word.split())  # âœ… ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        max_sim = max(
            jaccard_similarity(cons_word_set, set(user_word.split()))
            for user_word in user_keywords
        )
        if max_sim >= threshold:  # âœ… ì¼ì • ìœ ì‚¬ë„ ì´ìƒì´ë©´ ì¶”ê°€
            filtered_keywords.add(cons_word)

    return list(filtered_keywords)  # âœ… ìµœì¢… í•„í„°ë§ëœ í‚¤ì›Œë“œ ë°˜í™˜


def adjust_faiss_keywords(user_input, faiss_keywords):
    """ìœ ì € ì…ë ¥ í‚¤ì›Œë“œì™€ FAISS í‚¤ì›Œë“œë¥¼ ëª¨ë‘ í¬í•¨í•˜ì—¬ ê²€ìƒ‰"""
    user_keywords = extract_keywords(user_input, top_k=5)  # âœ… ìœ ì € ì…ë ¥ í‚¤ì›Œë“œ ì¶”ì¶œ

    # âœ… ìœ ì € ì…ë ¥ í‚¤ì›Œë“œ + FAISS í‚¤ì›Œë“œ í†µí•© (ì¤‘ë³µ ì œê±°)
    adjusted_keywords = list(set(user_keywords + faiss_keywords))

    print(f"âœ… [ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ]: {adjusted_keywords}")
    return adjusted_keywords


def extract_top_keywords_faiss(user_input, faiss_db, top_k=5):
    """FAISS ê²€ìƒ‰ í›„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ (ìœ ì € ì…ë ¥ ë°˜ì˜)"""
    print(f"ğŸ” [FAISS í‚¤ì›Œë“œ ì¶”ì¶œ] ì…ë ¥: {user_input}")

    search_results = faiss_db.similarity_search(user_input, k=15)
    all_text = " ".join([doc.page_content for doc in search_results])

    faiss_keywords = extract_keywords(all_text, top_k)  # âœ… FAISSì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œ

    adjusted_keywords = adjust_faiss_keywords(
        user_input, faiss_keywords
    )  # âœ… ìœ ì € í‚¤ì›Œë“œ ë°˜ì˜
    print(f"âœ… [FAISS ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ] {adjusted_keywords}")
    return adjusted_keywords


langchain_retriever = LangChainRetrieval()

# âœ… BART ëª¨ë¸ ê²½ë¡œ
MODEL_PATH = "./app/chatbot/model/1_bart/checkpoint-26606"

# âœ… ì „ì—­ ìºì‹±
bart_model = None
bart_tokenizer = None


def load_bart():
    """KoBART ëª¨ë¸ ë¡œë“œ (ì „ì—­ ìºì‹± ì ìš©)"""
    global bart_model, bart_tokenizer
    if bart_model is None or bart_tokenizer is None:
        try:
            print("ğŸ” KoBART ëª¨ë¸ ë¡œë“œ ì¤‘...")
            bart_model = BartForConditionalGeneration.from_pretrained(
                get_pytorch_kobart_model()
            )
            bart_tokenizer = get_kobart_tokenizer()
            bart_tokenizer.pad_token_id = bart_tokenizer.eos_token_id
            bart_tokenizer.model_max_length = 1024

            # âœ… `model.safetensors`ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
            state_dict = load_file(os.path.join(MODEL_PATH, "model.safetensors"))
            bart_model.load_state_dict(state_dict, strict=False)

            # âœ… ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì „í™˜
            bart_model.eval()
            print("âœ… KoBART ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ [KoBART ë¡œë“œ ì˜¤ë¥˜] {e}")
            bart_model, bart_tokenizer = None, None
    return bart_tokenizer, bart_model


def summarize_case(text, tokenizer, model):
    """íŒë¡€ ìš”ì•½: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ê¸¸ì–´ì•¼ ìš”ì•½ì„ ìˆ˜í–‰í•˜ë„ë¡ í•¨"""
    try:
        char_length = len(text)
        word_count = len(text.split())
        print(
            f"ğŸ” [DEBUG] ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ììˆ˜): {char_length}, ë‹¨ì–´ ìˆ˜: {word_count}"
        )

        if word_count < 5 or char_length < 20:
            return "ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì§§ì•„ ìš”ì•½ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # âœ… í† í°í™” í›„ ì¸ì½”ë”©ëœ ê°’ í™•ì¸ (clamp ì œê±° & padding ì¶”ê°€)
        input_ids = tokenizer.encode(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,  # âœ… íŒ¨ë”© ì¶”ê°€ë¡œ ì•ˆì •ì  í† í° ìƒì„±
        )
        print(f"ğŸ” [DEBUG] BART input_ids: {input_ids}")

        print("ğŸš€ [INFO] `generate()` ì‹¤í–‰ ì‹œì‘")

        summary_ids = model.generate(
            input_ids,
            max_length=200,  # âœ… ìµœëŒ€ ê¸¸ì´ ì¦ê°€ (150 â†’ 200)
            min_length=100,  # âœ… ìµœì†Œ ê¸¸ì´ ì¤„ì„ (149 â†’ 100)
            num_beams=3,  # âœ… beams ê°ì†Œë¡œ ì†ë„ ì¦ê°€ (5 â†’ 3)
            early_stopping=True,
            no_repeat_ngram_size=5,  # âœ… ë°˜ë³µ ë°©ì§€ (4 â†’ 5)
            repetition_penalty=2.5,  # âœ… ë°˜ë³µ ìµœì†Œí™” (2.2 â†’ 2.5)
            length_penalty=0.8,  # âœ… ê¸¸ì´ ì œí•œ ì™„í™” (1.0 â†’ 0.8)
        )

        print(f"ğŸ” [DEBUG] summary_ids: {summary_ids}")

        decoded = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        print(f"ğŸ” [DEBUG] ìš”ì•½ ê²°ê³¼: {decoded}")

        return decoded

    except Exception as e:
        print(f"âŒ [íŒë¡€ ìš”ì•½ ì˜¤ë¥˜] {e}")
        return "âŒ ìš”ì•½ ì‹¤íŒ¨"



@lru_cache(maxsize=1000)
def get_bart_model():
    return load_bart()

executor = ThreadPoolExecutor(max_workers=10)



async def search(query: str):
    """FAISS + SQL + ë²•ë¥  ìƒë‹´ & íŒë¡€ ê²€ìƒ‰ ìµœì í™” (ë¹„ë™ê¸° ì ìš©)"""
    print(f"\nğŸ” [INFO] ê²€ìƒ‰ ì‹¤í–‰: {query}")

    # âœ… 1. FAISS ë¡œë“œ
    faiss_db = load_faiss()
    if not faiss_db:
        print("âŒ [ì˜¤ë¥˜] FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"error": "FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    # âœ… 2. ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœëŒ€ 5ê°œ ì‚¬ìš©)
    search_keywords = extract_top_keywords_faiss(query, faiss_db, top_k=5)
    print(f"âœ… [ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ]: {search_keywords}")

    # âœ… 3. ë²•ë¥  ìƒë‹´ ë°ì´í„° ê²€ìƒ‰
    (
        consultation_results,
        consultation_categories,
        consultation_titles,
    ) = await async_search_consultation(search_keywords)

    if not consultation_results:
        print("âŒ [SQL ê²€ìƒ‰ ì‹¤íŒ¨] í•´ë‹¹ ìƒë‹´ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {
            "search_result": "í•´ë‹¹ ìƒë‹´ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "keywords_used": search_keywords,
            "consultation_result": "ë²•ë¥  ìƒë‹´ ê²€ìƒ‰ ì‹¤íŒ¨",
            "precedent_detail": "ì—†ìŒ",
            "summary": "BART ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ",
            "bert_prediction": "BERT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ",
            "final_answer": "ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        }

    # âœ… 4. ìƒë‹´ ê¸°ë°˜ íŒë¡€ ê²€ìƒ‰
    precedent_results = await async_search_precedent(
        consultation_categories,
        consultation_titles,
        search_keywords,
    )

    if not precedent_results:
        print("âŒ [SQL ê²€ìƒ‰ ì‹¤íŒ¨] í•´ë‹¹ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {
            "search_result": "í•´ë‹¹ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "keywords_used": search_keywords,
            "consultation_result": consultation_results,
            "precedent_detail": "ì—†ìŒ",
            "summary": "BART ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ",
            "bert_prediction": "BERT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ",
            "final_answer": "ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        }

    # âœ… 5. ê°€ì¥ ì—°ê´€ì„± ë†’ì€ íŒë¡€ ì„ íƒ
    most_relevant_precedent = precedent_results[0]
    print(f"âœ… [ì„ íƒëœ íŒë¡€]: {most_relevant_precedent}")

    # âœ… 6. Tavily ìš”ì•½ ê²€ìƒ‰
    tavily_result, casenote_url = await search_tavily_for_precedents(
        most_relevant_precedent
    )

    # âœ… 7. íŒë¡€ ìƒì„¸ ì •ë³´ êµ¬ì„±
    precedent_detail = f"""
    ì‚¬ê±´ë²ˆí˜¸: {most_relevant_precedent["c_number"]}
    ì‚¬ê±´ì¢…ë¥˜: {most_relevant_precedent["c_type"]}
    íŒê²°ì¼: {most_relevant_precedent["j_date"]}
    ë²•ì›: {most_relevant_precedent["court"]}
    ë‚´ìš©ìš”ì•½: {tavily_result}
    ì›ë¬¸ ë§í¬: {casenote_url}
    """

    # âœ… 8. BART ìš”ì•½ ìƒì„± ì…ë ¥ ì¤€ë¹„
    selected_answers = "\n\n".join([c["answer"] for c in consultation_results[:2]])
    selected_consultations = "\n\n".join(
        [
            f"ID: {c['id']}, ì¹´í…Œê³ ë¦¬: {c['category']}, ì„œë¸Œ ì¹´í…Œê³ ë¦¬: {c['sub_category']}, ì œëª©: {c['title']}, ì§ˆë¬¸: {c['question']}"
            for c in consultation_results[:2]
        ]
    )
    summary_input = f"""
    [ìƒë‹´ ë‹µë³€ 2ê°œ]
    {selected_answers}

    [ìƒë‹´ ê²€ìƒ‰ ë°ì´í„° 2ê°œ]
    {selected_consultations}
    """

    # âœ… 9. BART ìš”ì•½ ìˆ˜í–‰
    summary = summarize_case(summary_input, *get_bart_model())
    print(f"âœ… [BART ìš”ì•½ ì™„ë£Œ] {summary[:100]}...")

    # âœ… 10. ìµœì¢… ë‹µë³€ ìƒì„±
    final_answer = langchain_retriever.generate_legal_answer(query, summary)
    print(f"âœ… [LLM ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ] {final_answer[:100]}...")

    # âœ… 11. ê²°ê³¼ ë°˜í™˜
    return {
        "search_result": precedent_results,
        "keywords_used": search_keywords,
        "consultation_result": consultation_results,
        "precedent_detail": precedent_detail,
        "summary": summary,
        "final_answer": final_answer,
        "tavily_result": tavily_result,
        "casenote_url": casenote_url,
    }


def main():
    """CLI ê¸°ë°˜ ë²•ë¥  AI ì‹¤í–‰"""
    print("âœ… [ì‹œì‘] ë²•ë¥  AI ì‹¤í–‰")

    load_faiss()
    get_bart_model()

    while True:
        user_query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ")
        if user_query.lower() == "exit":
            break

        # âœ… **ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰**
        result = asyncio.run(search(user_query))

        print("\nğŸ“Œ [SQL ê²€ìƒ‰ ê²°ê³¼]:", result["search_result"])
        print("\nğŸ“Œ [ì‚¬ìš©ëœ í‚¤ì›Œë“œ]:", result["keywords_used"])
        print("\nğŸ“Œ [ë²•ë¥  ìƒë‹´ ê²€ìƒ‰ ê²°ê³¼]:", result["consultation_result"])
        print("\nğŸ“Œ [ì„ íƒëœ íŒë¡€ ìƒì„¸ ì •ë³´]:", result["precedent_detail"])
        print("\nğŸ“Œ [BART ìš”ì•½]:", result["summary"])
        print("\nğŸ¤– [LLM ìµœì¢… ë‹µë³€]:", result["final_answer"])


if __name__ == "__main__":
    main()
