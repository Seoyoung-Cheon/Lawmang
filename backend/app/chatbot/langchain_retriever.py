import os
from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory  # âœ… LangChain ë©”ëª¨ë¦¬ ì¶”ê°€
from langchain_community.tools import TavilySearchResults
from langchain_teddynote import logging
from langchain_openai import ChatOpenAI
logging.langsmith("llamaproject")

import sys
import time


# "mistralai/Mistral-7B-Instruct-v0.3"
# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
# ----------------------------------------------------------#
HF_TOKEN = os.environ.get("HF_TOKEN")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
# ----------------------------------------------------------#
HUGGINGFACE_REPO_ID = "meta-llama/Llama-3.3-70B-Instruct"
#-----------------------------------------------------------#

def load_llm(use_chatgpt=True):
    """LLM ë¡œë“œ (HuggingFace Inference API)"""
    try:
        if use_chatgpt:
            print("ğŸ”¹ ChatGPT-3.5-Turbo ì‚¬ìš©")
            return ChatOpenAI(
                model="gpt-3.5-turbo",
                api_key=OPENAI_API_KEY,
                temperature=0.7,
                max_tokens=512,
                streaming=True,
            )
        else:
            print("ğŸ”¹ Mistral-7B ì‚¬ìš©")
            return HuggingFaceEndpoint(
                repo_id=HUGGINGFACE_REPO_ID,
                task="text-generation",
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,
                huggingfacehub_api_token=HF_TOKEN,
            )
    except Exception as e:
        print(f"âŒ [LLM ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None
    
class LawGoKRTavilySearch:
    """
    Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬ law.go.krì—ì„œë§Œ ê²€ìƒ‰í•˜ë„ë¡ ì œí•œí•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self, max_results=1):  # âœ… ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ ì¡°ì • ê°€ëŠ¥
        self.search_tool = TavilySearchResults(max_results=max_results)

    def run(self, query):
        """
        Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • URL(law.go.kr)ì—ì„œë§Œ ê²€ìƒ‰ ì‹¤í–‰
        """
        # âœ… íŠ¹ì • ì‚¬ì´íŠ¸(law.go.kr)ì—ì„œë§Œ ê²€ìƒ‰í•˜ë„ë¡ site í•„í„° ì ìš©
        site_restrict_query = f"site:law.go.kr {query}"

        try:
            # âœ… Tavily ê²€ìƒ‰ ì‹¤í–‰
            results = self.search_tool.run(site_restrict_query)

            # âœ… ê²°ê³¼ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            print("ğŸ” Tavily ì‘ë‹µ:", results)

            # âœ… ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
            if not isinstance(results, list):
                return (
                    f"âŒ Tavily ê²€ìƒ‰ ì˜¤ë¥˜: ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ({type(results)})"
                )

            # âœ… `law.go.kr`ì´ í¬í•¨ëœ ê²°ê³¼ë§Œ í•„í„°ë§
            filtered_results = [
                result
                for result in results
                if isinstance(result, dict)
                and "url" in result
                and "law.go.kr" in result["url"]
            ]

            # âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° ì²˜ë¦¬
            if not filtered_results:
                return "âŒ ê´€ë ¨ ë²•ë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            return filtered_results
        except Exception as e:
            return f"âŒ Tavily ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"


search_tool = LawGoKRTavilySearch(max_results=1)


class LangChainRetrieval:
    """LangChain ê¸°ë°˜ ë²•ë¥  ì‘ë‹µ ìƒì„± í´ë˜ìŠ¤"""

    def __init__(self):
        self.llm = load_llm()  # âœ… í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ìœ ì§€
        if not self.llm:
            print("âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # âœ… LangChain ë©”ëª¨ë¦¬ ì¶”ê°€ (ëŒ€í™” ê¸°ë¡ ì €ì¥)
        self.memory = ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        )
        # âœ… LangChain í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°)
        self.prompt_template = PromptTemplate(
            template="""
    You are a Korean legal expert.
    Answer the user's question concisely and clearly based on the given legal context.
    If the question is unrelated to law, reinterpret it from a legal perspective.
    
    Previous conversation history:
    {chat_history}
    
    The user's question is:
    "{user_query}"
    
    **Tavily Search Result:**
    {search_result}

   **AI-generated case summary **
    {summary}

    Now, based on both the Tavily search result and the AI-generated summary, provide a refined legal answer in fluent, formal Korean:
""",
            input_variables=[
                "chat_history",
                "user_query",
                "search_tool",
                "search_result",
                "summary",
            ],
        )

    def generate_legal_answer(self, user_query, summary):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë²•ë¥ ì  ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ìƒì„±"""
        if not self.llm:
            return "âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            chat_history = self.memory.load_memory_variables({}).get("chat_history", "")

            prompt = self.prompt_template.format(
                chat_history=chat_history,
                user_query=user_query,
                search_tool="Tavily Legal Search",
                search_result=search_tool.run(user_query),
                summary=summary,
            )

            # âœ… LLM ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‹¤í–‰ (í•œ ê¸€ìì”© ì¶œë ¥)
            print("\nAI: ", end="", flush=True)
            response = ""
            for chunk in self.llm.stream(prompt):
                chunk_text = chunk.content if hasattr(chunk, "content") else str(chunk)
                if chunk_text:
                    sys.stdout.write(chunk_text)
                    sys.stdout.flush()
                    response += chunk_text  # âœ… ì „ì²´ ì‘ë‹µ ì €ì¥

            print("\n")  # âœ… ì‘ë‹µì´ ëë‚˜ë©´ ê°œí–‰ ì¶”ê°€

            # âœ… ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
            self.memory.save_context({"user_query": user_query}, {"response": response})

            return response if response else "âŒ ì •ìƒì ì¸ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"âŒ LLM ì˜¤ë¥˜: {str(e)}"

class COD_Agent:
    """ë³´ì¡° CoD(Chain of Draft) AI"""

    def __init__(self):
        self.llm = load_llm()
        if not self.llm:
            print("âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.memory = ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        )

        self.prompt_template = PromptTemplate(
            template="""
    Think step by step, but only keep minimum draft for each thinking step, with 5 words at most.
    Return the "Yes" or "No" at the end of the response after a separator ####.
    
    Format: |
    Q: {question}
    A: {answer}
    
    Few-shot Examples:
    - Q: Is the following sentence plausible? â€œKyle Palmieri was called for slashing.â€
      A: Kyle: hockey; slashing: hockey. #### Yes
    - Q: Is the following sentence plausible? â€œJoao Moutinho caught the screen pass in the NFC championship.â€
      A: Joao: soccer; NFC: football. #### No

""",
            input_variables=["question", "answer"],  
        )

    def generate_COD_Agent(self, question):
        """CoD AI Response"""
        if not self.llm:
            return "COD AI Not responding."

        try:
            prompt = self.prompt_template.format(
                question=question,
                answer="",  # TODO: ì¡°ê¸ˆ ìˆ˜ì • 
            )

            response = self.llm.invoke(prompt).content  # 

            #  ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
            self.memory.save_context({"question": question}, {"answer": response})

            return response if response else "âŒ ì •ìƒì ì¸ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"âŒ LLM ì˜¤ë¥˜: {str(e)}"
        
        
class summary_agent:
    """ê´€ë ¨ ëª¨ë“  íŒë¡€ ìƒì„¸ ìš”ì•½"""

    def __init__(self):
        self.llm = load_llm()
        if not self.llm:
            print("âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.memory = ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        )

        self.prompt_template = PromptTemplate(
            template=""" Inspect "sorted_keywords:{sorted_keywords}" 
            Use"user_query:{user_query}" to identify and sub_category:{sub_category} to identify True or False
            Return the "True" or "False" at the end of the response after a separator ####.
                Calculation = jaccard_similarity(sorted_keywords + [True, False])
                
                False will only return #### False
                True will return #### True 
                
        Few-shot Examples:
        - sorted_keywords: 'ê²½ìš°', 'ìˆ˜ì‚°ë¬¼', 'ì‹í’ˆìœ„ìƒë²•', 'ë³´ê´€', 'ì‹í’ˆ'
        - user_query: ì°¸ì¹˜ ë§ˆìš” 
        - sub_category: ê³„ì•½ì˜ í•´ì§€ í•´ì œ
        - #### False
        
        Few-shot Examples:
        - sorted_keywords: 'íŒŒì‚°', 'ë¹š', 'ê±±ì •'
        - user_query: íŒŒì‚°í• ê²ƒ ê°™ìŠµë‹ˆë‹¤ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”
        - sub_category: 'íŒŒì‚°ì‹ ì²­' 
        - #### True 
    """,
            input_variables=["sorted_keywords", "user_query", "sub_category"],
        )

    def generate_summary_agent(self, question):
        """CoD AI Response"""
        if not self.llm:
            return "COD AI Not responding."

        try:
            prompt = self.prompt_template.format(
                question=question,
                answer="",  # TODO: ì¡°ê¸ˆ ìˆ˜ì •
            )

            response = self.llm.invoke(prompt).content  #

            #  ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
            self.memory.save_context({"question": question}, {"answer": response})

            return response if response else "âŒ ì •ìƒì ì¸ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"âŒ LLM ì˜¤ë¥˜: {str(e)}"


# class summary_agent:
#     """ê´€ë ¨ ëª¨ë“  íŒë¡€ ìƒì„¸ ìš”ì•½"""

#     def __init__(self):
#         self.llm = load_llm()
#         if not self.llm:
#             print("âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

#         self.memory = ConversationBufferMemory(
#             memory_key="chat_history", return_messages=True
#         )

#         self.prompt_template = PromptTemplate(
#             template="""
#     Think step by step, but only keep minimum draft for each thinking step, with 5 words at most.
#     Return the "Yes" or "No" at the end of the response after a separator ----.
    
#     Format: |
#     Q: {question}
#     A: {answer}
    
#     Few-shot Examples:
#     - Q: Is the following sentence plausible? â€œKyle Palmieri was called for slashing.â€
#       A: Kyle: hockey; slashing: hockey. #### Yes
#     - Q: Is the following sentence plausible? â€œJoao Moutinho caught the screen pass in the NFC championship.â€
#       A: Joao: soccer; NFC: football. #### No

#     Now, provide your brief analysis in Korean:
# """,
#             input_variables=["question", "answer", "sub_category"],
#         )

#     def generate_summary_agent(self, question):
#         """CoD AI Response"""
#         if not self.llm:
#             return "COD AI Not responding."

#         try:
#             prompt = self.prompt_template.format(
#                 question=question,
#                 answer="",  # TODO: ì¡°ê¸ˆ ìˆ˜ì •
#             )

#             response = self.llm.invoke(prompt).content  #

#             #  ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
#             self.memory.save_context({"question": question}, {"answer": response})

#             return response if response else "âŒ ì •ìƒì ì¸ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
#         except Exception as e:
#             return f"âŒ LLM ì˜¤ë¥˜: {str(e)}"

