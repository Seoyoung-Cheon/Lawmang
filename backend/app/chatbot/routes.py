# from fastapi import APIRouter, HTTPException
# from fastapi.responses import StreamingResponse
# from pydantic import BaseModel
# import asyncio
# import json

# from app.chatbot.main import (
#     load_faiss,
#     llm2_lock,
#     run_initial_controller,
#     run_full_consultation,
#     run_final_answer_generation,
# )
# from app.chatbot.tool_agents.utils.utils import faiss_kiwi

# router = APIRouter()


# class QueryRequest(BaseModel):
#     query: str


# @router.post("/search/stream")
# async def chat_stream(request: QueryRequest):
#     user_query = request.query.strip()
#     if not user_query:
#         raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

#     faiss_db = load_faiss()
#     if not faiss_db:
#         raise HTTPException(status_code=500, detail="FAISS ë¡œë“œ ì‹¤íŒ¨")

#     stop_event = asyncio.Event()
#     template_data = {}

#     # âœ… LLM1ê³¼ LLM2 ë³‘ë ¬ ì‹¤í–‰
#     initial_task = asyncio.create_task(
#         run_initial_controller(
#             user_query=user_query,
#             faiss_db=faiss_db,
#             current_yes_count=0,
#             template_data=template_data,
#             stop_event=stop_event,
#         )
#     )

#     build_task = asyncio.create_task(
#         run_full_consultation(
#             user_query,
#             faiss_kiwi.extract_top_keywords_faiss(user_query, faiss_db),  # ìœ„ì¹˜ ì¸ì
#             "gpt-4",
#             True,
#             stop_event,
#         )
#     )

#     async def event_generator():
#         try:
#             # ğŸ”„ ë¨¼ì € LLM1 ê²°ê³¼ë§Œ ë°›ì•„ì„œ ë°”ë¡œ yield
#             initial_result = await asyncio.shield(initial_task)  # LLM1 ë³´í˜¸ ì‹¤í–‰

#             llm1_filtered = {
#                 "mcq_question": initial_result.get("mcq_question"),
#                 "strategy_summary": initial_result.get("strategy_summary"),
#                 "precedent_summary": initial_result.get("precedent_summary"),
#                 "yes_count": initial_result.get("yes_count"),
#             }
#             yield json.dumps({"type": "llm1", "data": llm1_filtered}) + "\n"

#             # âœ… LLM2 ì¡°ê±´ ì¶©ì¡± ì‹œ ì‹¤í–‰ (YES â‰¥ 3)
#             if initial_result.get("yes_count", 0) >= 3:
#                 # LLM2 ëŒ€ê¸°
#                 prepared_data = await asyncio.shield(build_task)
#                 if not all(
#                     prepared_data.get(k) for k in ["template", "strategy", "precedent"]
#                 ):
#                     yield json.dumps({
#                         "type": "llm2",
#                         "error": "âš ï¸ ì „ëµ ë˜ëŠ” íŒë¡€ ìƒì„± ì‹¤íŒ¨"
#                     }) + "\n"
#                     return

#                 async with llm2_lock:
#                     final_answer = run_final_answer_generation(
#                         prepared_data["template"],
#                         prepared_data["strategy"],
#                         prepared_data["precedent"],
#                         user_query,
#                         model="gpt-4",
#                     )

#                 yield json.dumps({
#                     "type": "llm2",
#                     "data": {
#                         "final_answer": final_answer,
#                         "final_summary": prepared_data["template"]["summary"],
#                         "strategy_summary": prepared_data["strategy"]["final_strategy_summary"],
#                         "precedent_summary": prepared_data["precedent"]["summary"],
#                         "casenote_url": prepared_data["precedent"]["casenote_url"],
#                     },
#                 }) + "\n"

#         except Exception as e:
#             yield json.dumps({
#                 "type": "error",
#                 "message": f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {str(e)}"
#             }) + "\n" 

#     return StreamingResponse(event_generator(), media_type="text/event-stream")

# ---------------------------------

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import os
import sys
import asyncio
from asyncio import Lock
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from app.chatbot.tool_agents.executor.normalanswer import run_final_answer_generation
from app.chatbot.initial_agents.controller import run_initial_controller
from app.chatbot.tool_agents.controller import run_full_consultation
from app.chatbot.tool_agents.utils.utils import faiss_kiwi
from fastapi import FastAPI

# âœ… ë½: ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ (LLM2 ê´€ë ¨)
llm2_lock = Lock()
yes_count = 0
sys.path.append(os.path.abspath("."))
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_FAISS_PATH = "./app/chatbot/faiss"

app = FastAPI()

router = APIRouter()
yes_count_memory = {}  # ê°„ë‹¨í•œ ê¸€ë¡œë²Œ YES ë©”ëª¨ë¦¬ (ì„¸ì…˜/ìœ ì € êµ¬ë¶„ ê°€ëŠ¥ì‹œ í™•ì¥)


def load_faiss():
    try:
        embedding_model = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=OPENAI_API_KEY,
        )
        return FAISS.load_local(
            DB_FAISS_PATH,
            embedding_model,
            allow_dangerous_deserialization=True,
        )
    except Exception as e:
        # print(f"âŒ FAISS ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
class QueryRequest(BaseModel):
    query: str


# âœ… 1. LLM1: ì´ˆê¸° ì‘ë‹µë§Œ
@router.post("/initial")
async def chatbot_initial(request: QueryRequest):
    user_query = request.query.strip()
    if not user_query:
        raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    faiss_db = load_faiss()
    if not faiss_db:
        raise HTTPException(status_code=500, detail="FAISS ë¡œë“œ ì‹¤íŒ¨")

    stop_event = asyncio.Event()
    template_data = {}

    result = await run_initial_controller(
        user_query=user_query,
        faiss_db=faiss_db,
        current_yes_count=0,
        template_data=template_data,
        stop_event=stop_event,
    )

    # YES ì¹´ìš´íŠ¸ ì €ì¥
    yes_count_memory[user_query] = result.get("yes_count", 0)

    # íŒë¡€/ì „ëµì€ ë Œë”ë§í•˜ì§€ ì•ŠìŒ
    return {
        "mcq_question": result.get("mcq_question"),
        "yes_count": result.get("yes_count", 0),
        # í•„ìš”ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ
        # "strategy_summary": result.get("strategy_summary"),
        # "precedent_summary": result.get("precedent_summary"),
    }


# âœ… 2. LLM2 ë¹Œë“œ ì „ìš©: ì „ëµ/íŒë¡€ ìºì‹±ë§Œ ìˆ˜í–‰
@router.post("/prepare")
async def chatbot_prepare(request: QueryRequest):
    user_query = request.query.strip()
    faiss_db = load_faiss()
    if not faiss_db:
        raise HTTPException(status_code=500, detail="FAISS ë¡œë“œ ì‹¤íŒ¨")

    keywords = faiss_kiwi.extract_top_keywords_faiss(user_query, faiss_db)
    stop_event = asyncio.Event()

    # ì „ëµ + íŒë¡€ë§Œ ìƒì„± (GPT í˜¸ì¶œ ì—†ì´)
    await run_full_consultation(
        user_query=user_query,
        search_keywords=keywords,
        model="gpt-4",
        build_only=True,
        stop_event=stop_event,
    )

    return {"status": "ok", "message": "ë°±ê·¸ë¼ìš´ë“œ ë¹Œë“œ ì™„ë£Œ"}


# âœ… 3. LLM2 ìµœì¢… ì‘ë‹µ: ê³ ê¸‰ GPT ì‹¤í–‰
@router.post("/advanced")
async def chatbot_advanced(request: QueryRequest):
    user_query = request.query.strip()
    faiss_db = load_faiss()
    if not faiss_db:
        raise HTTPException(status_code=500, detail="FAISS ë¡œë“œ ì‹¤íŒ¨")

    keywords = faiss_kiwi.extract_top_keywords_faiss(user_query, faiss_db)
    stop_event = asyncio.Event()

    # ì „ëµ/íŒë¡€ + GPT ìµœì¢… ì‘ë‹µê¹Œì§€ ìƒì„±
    prepared_data = await run_full_consultation(
        user_query=user_query,
        search_keywords=keywords,
        model="gpt-4",
        build_only=False,
        stop_event=stop_event,
    )

    if not all(prepared_data.get(k) for k in ["template", "strategy", "precedent"]):
        raise HTTPException(status_code=500, detail="ì „ëµ ë˜ëŠ” íŒë¡€ ìƒì„± ì‹¤íŒ¨")

    async with llm2_lock:
        final_answer = run_final_answer_generation(
            template=prepared_data["template"],
            strategy=prepared_data["strategy"],
            precedent=prepared_data["precedent"],
            user_query=user_query,
            model="gpt-4",
        )

    return {
        "template": prepared_data["template"],
        "strategy": prepared_data["strategy"],
        "precedent": prepared_data["precedent"],
        "final_answer": final_answer,
        "status": "ok",
    }
