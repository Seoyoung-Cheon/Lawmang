from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from fastapi.responses import StreamingResponse
from app.chatbot.initial_agents.controller import run_initial_controller
from app.chatbot.tool_agents.controller import run_full_consultation
from app.chatbot.tool_agents.executor.normalanswer import run_final_answer_generation
from app.chatbot.tool_agents.utils.utils import faiss_kiwi
from app.chatbot.main import load_faiss, llm2_lock

import asyncio
import json

router = APIRouter()


class QueryRequest(BaseModel):
    query: str


@router.post("/search/stream")
async def streaming_chat(request: QueryRequest):
    faiss_db = load_faiss()
    if not faiss_db:
        raise HTTPException(status_code=500, detail="FAISS ë¡œë“œ ì‹¤íŒ¨")

    async def event_stream():
        try:
            keywords = faiss_kiwi.extract_top_keywords_faiss(request.query, faiss_db)

            # ğŸ”„ LLM2 ë°±ê·¸ë¼ìš´ë“œ ë¹Œë“œ
            llm2_task = asyncio.create_task(
                run_full_consultation(
                    user_query=request.query,
                    search_keywords=keywords,
                    build_only=True,
                    stop_event=None,
                )
            )

            # âœ… LLM1 ì‹¤í–‰
            llm1_result = await run_initial_controller(
                user_query=request.query,
                faiss_db=faiss_db,
                current_yes_count=0,
                template_data={},
                stop_event=None,
            )

            # 1ï¸âƒ£ ì´ˆê¸° ì‘ë‹µ ë¹ ë¥´ê²Œ ì¶œë ¥
            initial_response = llm1_result.get("initial_response", "").strip()
            yield (
                json.dumps({"type": "llm1", "data": {"text": initial_response}}) + "\n"
            )

            # 2ï¸âƒ£ í›„ì† ì§ˆë¬¸ (ì¡°ê¸ˆ ë‚˜ì¤‘ì— append=True)
            mcq_question = llm1_result.get("mcq_question", "").strip()
            if mcq_question:
                await asyncio.sleep(2)
                yield (
                    json.dumps(
                        {"type": "llm1", "data": {"text": mcq_question, "append": True}}
                    )
                    + "\n"
                )
                # âœ… LLM2 ì¡°ê±´ ì¶©ì¡± ì‹œ
                if llm1_result.get("yes_count", 0) >= 3:

                    # ğŸ”¹ ë¡œê·¸ ë©”ì‹œì§€: ì „ëµ ì„¤ê³„ ì‹œì‘
                    yield json.dumps({
                        "type": "log",
                        "message": "ğŸ“ ì „ëµ ì„¤ê³„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."
                    }) + "\n"

                    prepared = await llm2_task
                    template = prepared.get("template")
                    strategy = prepared.get("strategy")
                    precedent = prepared.get("precedent")

                    if not template or not strategy:
                        yield json.dumps({
                            "type": "llm2",
                            "error": "âš ï¸ í…œí”Œë¦¿ ë˜ëŠ” ì „ëµ ìƒì„± ì‹¤íŒ¨"
                        }) + "\n"
                        return

                    if not precedent:
                        yield json.dumps({
                            "type": "llm2",
                            "error": "âš ï¸ íŒë¡€ ê²€ìƒ‰ ì‹¤íŒ¨"
                        }) + "\n"
                        return

                    # ğŸ”¹ ë¡œê·¸ ë©”ì‹œì§€: GPT ì‘ë‹µ ì‹œì‘
                    yield json.dumps({
                        "type": "log",
                        "message": "ğŸ¤– ê³ ê¸‰ ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
                    }) + "\n"

                    async with llm2_lock:
                        final_answer = run_final_answer_generation(
                            template, strategy, precedent, request.query, model="gpt-4"
                        )

                    yield json.dumps({
                        "type": "llm2",
                        "data": {
                            "final_answer": final_answer,
                            "final_summary": template.get("summary", ""),
                            "strategy_summary": strategy.get("final_strategy_summary", ""),
                            "precedent_summary": precedent.get("summary", ""),
                            "casenote_url": precedent.get("casenote_url", ""),
                        },
                    }) + "\n"
                

        except Exception as e:
            yield (
                json.dumps(
                    {"type": "error", "message": f"ìŠ¤íŠ¸ë¦¬ë° ë„ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
                )
                + "\n"
            )

    return StreamingResponse(event_stream(), media_type="text/event-stream")
