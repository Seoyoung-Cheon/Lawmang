import os
import json
import asyncio
from typing import Optional, Dict, Any
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from app.chatbot.tool_agents.tools import LawGoKRTavilySearch
from app.chatbot.tool_agents.utils.utils import insert_hyperlinks_into_text
from app.chatbot.memory.global_cache import memory  # ConversationBufferMemory ì¸ìŠ¤í„´ìŠ¤
from app.chatbot.initial_agents.initial_chatbot import LegalChatbot

# ê¸€ë¡œë²Œ ìºì‹œ ê¸°ëŠ¥: í…œí”Œë¦¿ì„ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì €ì¥í•˜ê³  ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜ë“¤
from app.chatbot.memory.global_cache import (
    retrieve_template_from_memory,
)

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def load_llm():
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        temperature=0.3,
        max_tokens=1024,
    )


class AskHumanAgent:
    def __init__(self):
        self.llm = load_llm()
        self.tavily_search = LawGoKRTavilySearch()

    async def build_mcq_prompt_full(
        self,
        user_query,
        llm1_answer,
        template_data,
        yes_count,
        report: Optional[str] = None,  # âœ… ë³´ê³ ì„œ ì¶”ê°€
    ):
        template = template_data.get("template", {}) if template_data else {}
        strategy = template_data.get("strategy", {}) if template_data else {}
        precedent = template_data.get("precedent", {}) if template_data else {}

        summary_with_links = insert_hyperlinks_into_text(
            template.get("summary", ""), template.get("hyperlinks", [])
        )
        explanation_with_links = insert_hyperlinks_into_text(
            template.get("explanation", ""), template.get("hyperlinks", [])
        )
        hyperlinks_text = "\n".join(
            f"- {link['label']}: {link['url']}"
            for link in template.get("hyperlinks", [])
        )
        strategy_decision_tree = "\n".join(strategy.get("decision_tree", []))
        precedent_summary = precedent.get("summary", "íŒë¡€ ìš”ì•½ ì—†ìŒ")
        precedent_link = precedent.get("casenote_url", "ë§í¬ ì—†ìŒ")
        precedent_meta = f"{precedent.get('court', '')} / {precedent.get('j_date', '')} / {precedent.get('title', '')}"

        memory.load_memory_variables({}).get("chat_history", "")

        prompt = f"""
        ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ì„ ìƒì„±í•˜ëŠ” ê³ ê¸‰ AIì…ë‹ˆë‹¤.

        ì•„ë˜ëŠ” 1ì°¨ íŒë‹¨ ê²°ê³¼ë¡œ ìƒì„±ëœ ì‹¤ì‹œê°„ ë³´ê³ ì„œì…ë‹ˆë‹¤.  
        **ì´ ë³´ê³ ì„œëŠ” ë°˜ë“œì‹œ í›„ì† ìƒë‹´ì˜ í•µì‹¬ ê·¼ê±°ë¡œ ì‚¼ì•„ì•¼ í•˜ë©°**,  
        ìš”ì•½Â·ì„¤ëª…Â·ì „ëµ ë“±ì€ ëª¨ë‘ ì´ ë³´ê³ ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

            

    ğŸ“ [ì‹¤ì‹œê°„ íŒë‹¨ ë³´ê³ ì„œ]
    {report.strip() if report else "ë³´ê³ ì„œ ì—†ìŒ"}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {user_query}

    [ìš”ì•½]
    {summary_with_links}

    [ì„¤ëª…]
    {explanation_with_links}

    [ì°¸ê³  ì§ˆë¬¸]
    {template.get("ref_question", "í•´ë‹¹ ì—†ìŒ")}

    [í•˜ì´í¼ë§í¬]
    {hyperlinks_text}

    [ì „ëµ ìš”ì•½]
    {strategy.get("final_strategy_summary", "")}

    [ì‘ë‹µ êµ¬ì„± ì „ëµ]
    - ë§íˆ¬: {strategy.get("tone", "")}
    - íë¦„: {strategy.get("structure", "")}
    - ì¡°ê±´ íë¦„ë„:
    {strategy_decision_tree}

    [ì¶”ì²œ ë§í¬]
    {json.dumps(strategy.get("recommended_links", []), ensure_ascii=False)}

    [ì¶”ê°€ëœ íŒë¡€ ìš”ì•½]
    - {precedent_summary}
    - ë§í¬: {precedent_link}
    - ì •ë³´: {precedent_meta}
 ğŸ¯ ì‘ì—…:
 - ë°˜ë“œì‹œ [ì‹¤ì‹œê°„ íŒë‹¨ ë³´ê³ ì„œ] ë‚´ìš©ì„ ìµœìš°ì„  íŒë‹¨ ê·¼ê±°ë¡œ ì‚¼ê³ , ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë²•ë¥  ìƒë‹´ì„ ìƒì„±í•˜ì„¸ìš”.
 - ì´ì „ ëŒ€í™”ì™€ ì´ì–´ì§€ëŠ” ìœ„ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬, ì‚¬ìš©ìê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë²•ë¥  ìƒë‹´ì„ ìƒì„±í•˜ì„¸ìš”.
 - ê° í•­ëª©ì€ ì‹¤ì œ ìƒí™©ì„ ë°˜ì˜í•˜ë©°, ì‚¬ìš©ìê°€ ìì‹ ì˜ ìƒí™©ì— ë§ëŠ” ì„ íƒì§€ë¥¼ ì´í•´í•  ìˆ˜ ìˆê²Œ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    """
        return prompt

    async def generate_mcq_question(
        self,
        user_query,
        llm1_answer,
        yes_count=0,
        template_data=None,
        report: Optional[str] = None,
    ):
        prompt = await self.build_mcq_prompt_full(
            user_query,
            llm1_answer,
            template_data or {},
            yes_count,
            report,  # âœ… ì „ë‹¬
        )
        response = await self.llm.ainvoke(prompt)
        return response.content.strip()
    
        
    async def ask_human(
        self,
        user_query,
        llm1_answer,
        current_yes_count=0,
        template_data=None,
        initial_response: Optional[str] = None,  # âœ… ë³´ê³ ì„œ ì§ì ‘ ì£¼ì…
    ):
        # ìºì‹œëœ í…œí”Œë¦¿ í™•ì¸
        cached_data = retrieve_template_from_memory()
        if cached_data and cached_data.get("built", False):
            template_data = cached_data

        # YES count ëˆ„ì 
        yes_count_detected = 1 if "###yes" in llm1_answer.lower() else 0
        total_yes_count = current_yes_count + yes_count_detected

        # í›„ì† ì§ˆë¬¸ ìƒì„±
        mcq_q = await self.generate_mcq_question(
            user_query,
            llm1_answer,
            total_yes_count,
            template_data,
            report=initial_response,  # âœ… ì „ë‹¬
        )

        # í…œí”Œë¦¿ ì‚¬ìš© í‘œê¸°
        if total_yes_count >= 2:
            mcq_q = f"{mcq_q}\n\n[ì €ì¥ëœ í…œí”Œë¦¿ ì‚¬ìš©ë¨]"

        # âœ… ë³´ê³ ì„œê°€ ìˆìœ¼ë©´ ë¶™ì—¬ì„œ ë°˜í™˜
        if initial_response:
            combined = f"{initial_response.strip()}\n\nğŸ§© [í›„ì† ì§ˆë¬¸ ì œì•ˆ]\n{mcq_q.strip()}"
        else:
            combined = mcq_q.strip()

        return {
            "yes_count": total_yes_count,
            "mcq_question": combined,
            "is_mcq": True,
            "load_template_signal": total_yes_count >= 2,
        }


    def check_user_wants_advanced_answer(user_query: str) -> bool:
        keywords = [
            "ê³ ê¸‰ ë‹µë³€",
            "ìƒì„¸í•œ ì„¤ëª…",
            "ìì„¸íˆ ì•Œë ¤ì¤˜",
            "gpt-4",
            "íŒë¡€ê¹Œì§€",
            "ì „ëµë„",
            "ê³ ê¸‰ AI",
        ]
        return any(k in user_query.lower() for k in keywords)
