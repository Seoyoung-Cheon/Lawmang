import os
import re
import asyncio
from typing import Optional, Dict, Any
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from app.chatbot.tool_agents.tools import LawGoKRTavilySearch

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def load_llm():
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        temperature=0.6,
        max_tokens=1024,
    )


class AskHumanAgent:
    def __init__(self):
        self.llm_simple = load_llm()
        self.llm_mcq = load_llm()
        self.tavily_search = LawGoKRTavilySearch()

    def build_followup_prompt_ko(
        self, user_query: str, llm1_answer: str, yes_count: int
    ) -> str:
        return f"""
ë‹¹ì‹ ì€ ë²•ë¥  ë³´ì¡° AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

â“ ì‚¬ìš©ì ì§ˆë¬¸:
{user_query}

ğŸ’¬ ì´ì „ AI ì‘ë‹µ:
{llm1_answer}

ğŸ“Œ í˜„ì¬ê¹Œì§€ í™•ì¸ëœ ###yes ì¹´ìš´íŠ¸: {yes_count}

ğŸ¯ ì‘ì—…:
ì‚¬ìš©ìê°€ ë” ëª…í™•í•œ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

í˜•ì‹:
í›„ì† ì§ˆë¬¸: [ì§ˆë¬¸]
"""

    def build_mcq_prompt_with_precedent(
        self,
        user_query: str,
        llm1_answer: str,
        precedent_summary: str,
        strategy_summary: str = "",
        yes_count: int = 0,
    ) -> str:
        return f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ë³´ì¡° AIì…ë‹ˆë‹¤. íŒë¡€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì‹ ì§ˆë¬¸ì„ ìƒì„±í•˜ì—¬ ì‚¬ìš©ìì˜ ì¶”ê°€ ì •ë³´ë¥¼ ìœ ë„í•©ë‹ˆë‹¤.

â“ ì‚¬ìš©ì ì§ˆë¬¸:
{user_query}

ğŸ’¬ ì´ì „ AI ì‘ë‹µ:
{llm1_answer}

ğŸ“š ê²€ìƒ‰ëœ íŒë¡€ ìš”ì•½:
{precedent_summary}

ğŸ§  ì „ëµ ìš”ì•½ (ì„ íƒì ):
{strategy_summary or "í•´ë‹¹ ì—†ìŒ"}

ğŸ“Œ í˜„ì¬ê¹Œì§€ í™•ì¸ëœ ###yes ì¹´ìš´íŠ¸: {yes_count}

ğŸ¯ ì‘ì—…:
ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ê°ê´€ì‹ ì§ˆë¬¸ 1ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì§ˆë¬¸: [ê°ê´€ì‹ ì§ˆë¬¸ í…ìŠ¤íŠ¸]
A. [ì„ íƒì§€ A]
B. [ì„ íƒì§€ B]
C. [ì„ íƒì§€ C]
D. [ì„ íƒì§€ D]
ì •ë‹µ: [A/B/C/D]

ì¡°ê±´:
- ì„ íƒì§€ëŠ” 4ê°œë¡œ ëª…í™•íˆ ì‘ì„±.
- íŒë¡€ ë‚´ìš© ë°˜ë“œì‹œ í¬í•¨.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ë°€ì ‘í•œ ê´€ë ¨ ë‚´ìš© í¬í•¨.
"""
    async def generate_followup_question(
        self,
        user_query: str,
        llm1_answer: str,
        current_yes_count: int = 0,
    ) -> dict:
        yes_count_detected = len(re.findall(r"###yes", llm1_answer, flags=re.IGNORECASE))
        total_yes_count = current_yes_count + yes_count_detected

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.build_followup_prompt_ko(user_query, llm1_answer, total_yes_count)

        try:
            response = await self.llm_simple.ainvoke(prompt)
            followup_question = response.content.strip()

            load_template_signal = total_yes_count in [2, 3]

            if total_yes_count >= 3:
                total_yes_count = 0  # reset counter

            # âœ… debug_prompt ì¶”ê°€ ë°˜í™˜
            return {
                "followup_question": followup_question,
                "yes_count": total_yes_count,
                "is_mcq": False,
                "load_template_signal": load_template_signal,
                "debug_prompt": prompt,  # ë””ë²„ê·¸ìš© í”„ë¡¬í”„íŠ¸ ë°˜í™˜
            }
        except Exception as e:
            return {
                "followup_question": "í›„ì† ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                "error": str(e),
                "yes_count": current_yes_count,
                "is_mcq": False,
                "load_template_signal": False,
                "debug_prompt": prompt,  # ì˜ˆì™¸ ìƒí™©ì—ì„œë„ ë°˜í™˜
            }

    async def generate_mcq_question(
        self,
        user_query: str,
        llm1_answer: str,
        current_yes_count: int = 0,
        template_data: Optional[Dict[str, Any]] = None,
    ) -> dict:
        tavily_results = await asyncio.to_thread(self.tavily_search.run, user_query)
        precedent_summary = (
            tavily_results[0].get("content", "íŒë¡€ ìš”ì•½ ì—†ìŒ")
            if isinstance(tavily_results, list) and tavily_results
            else "ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )

        strategy_summary = (
            template_data.get("strategy", {}).get("final_strategy_summary", "")
            if template_data
            else ""
        )

        prompt = self.build_mcq_prompt_with_precedent(
            user_query=user_query,
            llm1_answer=llm1_answer,
            precedent_summary=precedent_summary,
            strategy_summary=strategy_summary,
            yes_count=current_yes_count,
        )

        try:
            response = await self.llm_mcq.ainvoke(prompt)
            mcq_content = response.content.strip()

            question_match = re.search(r"ì§ˆë¬¸:\s*(.+?)\nA\.", mcq_content, re.DOTALL)
            options_match = re.findall(r"([ABCD])\.\s*(.+)", mcq_content)
            answer_match = re.search(r"ì •ë‹µ:\s*([ABCD])", mcq_content)

            if question_match and options_match and answer_match:
                question_text = question_match.group(1).strip()
                options_dict = {label: option.strip() for label, option in options_match}
                correct_answer = answer_match.group(1)

                formatted_mcq = {
                    "question": question_text,
                    "options": options_dict,
                    "correct_answer": correct_answer,
                }

                load_template_signal = current_yes_count in [2, 3]

                return {
                    "followup_question": formatted_mcq,
                    "yes_count": 0 if current_yes_count >= 3 else current_yes_count,
                    "is_mcq": True,
                    "load_template_signal": load_template_signal,
                    "precedent_summary": precedent_summary,
                    "strategy_summary": strategy_summary,
                    "debug_prompt": prompt,  # âœ… MCQ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ ì¶”ê°€
                }
            else:
                return {
                    "followup_question": "ê°ê´€ì‹ ì§ˆë¬¸ ìƒì„± í¬ë§· ì˜¤ë¥˜ ë°œìƒ",
                    "error": "ì‘ë‹µ í¬ë§· ì˜¤ë¥˜",
                    "yes_count": current_yes_count,
                    "is_mcq": True,
                    "load_template_signal": False,
                    "debug_prompt": prompt,  # âœ… ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ MCQ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
                }

        except Exception as e:
            return {
                "followup_question": "ê°ê´€ì‹ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                "error": str(e),
                "yes_count": current_yes_count,
                "is_mcq": True,
                "load_template_signal": False,
                "debug_prompt": prompt,  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ MCQ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
            }

    # í†µí•© ë©”ì„œë“œ (yes countì— ë”°ë¼ ìë™ ì„ íƒ)
    async def ask_human(
        self,
        user_query: str,
        llm1_answer: str,
        current_yes_count: int = 0,
        template_data: Optional[Dict[str, Any]] = None,
    ) -> dict:
        yes_count_detected = len(re.findall(r"###yes", llm1_answer, flags=re.IGNORECASE))
        total_yes_count = current_yes_count + yes_count_detected

        if total_yes_count >= 2:  # âœ… YES ëˆ„ì ì´ 2 ì´ìƒì¼ ê²½ìš°ë§Œ íŒë¡€ ê¸°ë°˜ MCQ í˜¸ì¶œ
            return await self.generate_mcq_question(
                user_query, llm1_answer, total_yes_count, template_data
            )
        else:
            return await self.generate_followup_question(
                user_query, llm1_answer, total_yes_count
            )


def check_user_wants_advanced_answer(user_query: str) -> bool:
    keywords = [
        "ê³ ê¸‰ ë‹µë³€",
        "ìƒì„¸í•œ ì„¤ëª…",
        "ìì„¸íˆ ì•Œë ¤ì¤˜",
        "gpt-4",
        "íŒë¡€ê¹Œì§€",
        "ì „ëµë„",
        "ê³ ê¸‰ AI",
    ]
    return any(k in user_query.lower() for k in keywords)