import os
import re
import time
import asyncio
from typing import Optional, Dict, Any
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from app.chatbot.tool_agents.tools import LawGoKRTavilySearch

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def load_llm():
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        temperature=0.6,
        max_tokens=1024,
    )

class AskHumanAgent:
    def __init__(self):
        self.llm = load_llm()
        self.tavily_search = LawGoKRTavilySearch()

    def build_followup_prompt_ko(self, user_query, llm1_answer, yes_count):
        return f"""
ë‹¹ì‹ ì€ ë²•ë¥  ë³´ì¡° AIì…ë‹ˆë‹¤...

â“ ì‚¬ìš©ì ì§ˆë¬¸:
{user_query}

ğŸ’¬ ì´ì „ AI ì‘ë‹µ:
{llm1_answer}

ğŸ“Œ í˜„ì¬ê¹Œì§€ í™•ì¸ëœ ###yes ì¹´ìš´íŠ¸: {yes_count}

ğŸ¯ ì‘ì—…:
ì‚¬ìš©ìê°€ ë” ëª…í™•í•œ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

í˜•ì‹:
í›„ì† ì§ˆë¬¸: [ì§ˆë¬¸]
"""

    def build_mcq_prompt_with_precedent(
        self,
        user_query,
        llm1_answer,
        precedent_summary,
        strategy_summary="",
        yes_count=0,
    ):
        return f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ë³´ì¡° AIì…ë‹ˆë‹¤.

ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ **ê´€ë ¨ ì‚¬ë¡€ 4ê°€ì§€**ë¥¼ ì œì‹œí•˜ì„¸ìš”.

â“ ì‚¬ìš©ì ì§ˆë¬¸:
{user_query}

ğŸ’¬ ì´ì „ AI ì‘ë‹µ:
{llm1_answer}

ğŸ“š ê²€ìƒ‰ëœ íŒë¡€ ìš”ì•½:
{precedent_summary}

ğŸ§  ì „ëµ ìš”ì•½:
{strategy_summary or "í•´ë‹¹ ì—†ìŒ"}

ğŸ“Œ í˜„ì¬ ###yes ì¹´ìš´íŠ¸: {yes_count}

ğŸ¯ ì‘ì—…:
- ê° ì‚¬ë¡€ë¥¼ A, B, C, D í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
- ì„ íƒì§€ì²˜ëŸ¼ ë³´ì´ë˜, ì‹¤ì œë¡œëŠ” ê´€ë ¨ ì‚¬ë¡€ ì•ˆë‚´ì…ë‹ˆë‹¤.
- ê° í•­ëª©ì€ êµ¬ì²´ì ì´ê³  ì‹¤ì§ˆì ì¸ ìƒí™©ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
"""

    async def generate_followup_question(self, user_query, llm1_answer, yes_count=0):
        prompt = self.build_followup_prompt_ko(user_query, llm1_answer, yes_count)
        response = await self.llm.ainvoke(prompt)
        return response.content.strip()

    async def generate_mcq_question(
        self, user_query, llm1_answer, yes_count=0, template_data=None
    ):
        tavily_results = await asyncio.to_thread(self.tavily_search.run, user_query)
        precedent_summary = (
            tavily_results[0].get("content", "íŒë¡€ ìš”ì•½ ì—†ìŒ")
            if isinstance(tavily_results, list) and tavily_results
            else "ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
        strategy_summary = (
            template_data.get("strategy", {}).get("final_strategy_summary", "")
            if template_data
            else ""
        )
        precedent_summary = (
            template_data.get("precedent", {}).get("summary", precedent_summary)
            if template_data
            else precedent_summary
        )
        prompt = self.build_mcq_prompt_with_precedent(
            user_query, llm1_answer, precedent_summary, strategy_summary, yes_count
        )
        response = await self.llm.ainvoke(prompt)
        return response.content.strip()

    async def ask_human(
        self, user_query, llm1_answer, current_yes_count=0, template_data=None
    ):
        yes_count_detected = 1 if "###yes" in llm1_answer.lower() else 0
        total_yes_count = current_yes_count + yes_count_detected

        print("\nğŸ¤– AI: ë” ëª…í™•í•œ ì •ë³´ë¥¼ ìœ„í•´ í›„ì† ì§ˆë¬¸ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...\n")
        await asyncio.sleep(2)

        followup_q = await self.generate_followup_question(
            user_query, llm1_answer, total_yes_count
        )
        print("ğŸŸ¢ ì¼ë°˜ í›„ì† ì§ˆë¬¸:")
        print(followup_q)
        await asyncio.sleep(2)

        print("\nğŸ“¡ [íŒë¡€ ì •ë³´ë¥¼ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤...]\n")
        await asyncio.sleep(2)
        print("ğŸ§  [ì „ëµ ìš”ì•½ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...]\n")
        await asyncio.sleep(2)
        print("ğŸ“˜ [ì‚¬ë¡€ë¥¼ ì •ë¦¬í•˜ì—¬ ê°ê´€ì‹ ì§ˆë¬¸ì„ êµ¬ì„± ì¤‘ì…ë‹ˆë‹¤...]\n")
        await asyncio.sleep(2)

        mcq_q = await self.generate_mcq_question(
            user_query, llm1_answer, total_yes_count, template_data
        )
        print("ğŸŸ¦ ì‚¬ë¡€ ê¸°ë°˜ ê°ê´€ì‹ ì§ˆë¬¸:")
        print(mcq_q)

        return {
            "yes_count": total_yes_count,
            "followup_question": followup_q,
            "mcq_question": mcq_q,
            "is_mcq": True,
            "load_template_signal": total_yes_count in [2, 3],
        }


def check_user_wants_advanced_answer(user_query: str) -> bool:
    keywords = [
        "ê³ ê¸‰ ë‹µë³€",
        "ìƒì„¸í•œ ì„¤ëª…",
        "ìì„¸íˆ ì•Œë ¤ì¤˜",
        "gpt-4",
        "íŒë¡€ê¹Œì§€",
        "ì „ëµë„",
        "ê³ ê¸‰ AI",
    ]
    return any(k in user_query.lower() for k in keywords)
