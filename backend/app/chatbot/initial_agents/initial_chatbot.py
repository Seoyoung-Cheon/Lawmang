import os
import re
import sys
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from app.chatbot.tool_agents.utils.utils import faiss_kiwi, classify_legal_query
from app.chatbot.initial_agents.ask_human_for_info import (
    ask_human_for_information,
    check_user_wants_advanced_answer,
)

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def load_llm():
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        temperature=0.6,
        max_tokens=512,
        streaming=True,
    )


class LegalChatbot:
    def __init__(self, faiss_db):
        self.llm = load_llm()
        self.memory = ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        )
        self.faiss_db = faiss_db
        self.prompt_template = PromptTemplate(
            template="""
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì•„ëž˜ í‚¤ì›Œë“œ ë° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë²•ë¥ ì  ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ðŸ’¬ ëŒ€í™” ê¸°ë¡:
{chat_history}

â“ ì‚¬ìš©ìž ì§ˆë¬¸:
"{user_query}"

ðŸ§  ì‚¬ìš©ìž ìž…ë ¥ í‚¤ì›Œë“œ:
{query_keywords}

ðŸ“š FAISS ìœ ì‚¬ í‚¤ì›Œë“œ:
{faiss_keywords}

ðŸ“‚ ì§ˆë¬¸ ìœ í˜•: {query_type}
âš–ï¸ ë²•ë¥  ì—°ê´€ì„± ì ìˆ˜: {legal_score}

ðŸ“¢ ì§€ì‹œì‚¬í•­:
- ì§ˆë¬¸ ìœ í˜•ì´ **"legal"** ì´ë©´ â†’ ëª…í™•í•œ ë²•ë¥  ì¡°í•­ ë˜ëŠ” íŒë¡€ì— ê¸°ë°˜í•´ íŒë‹¨ì„ ë‚´ë ¤ì£¼ì„¸ìš”.
- ì§ˆë¬¸ ìœ í˜•ì´ **"nonlegal"** ì´ë©´ â†’ ë²•ì  ê´€ë ¨ì„±ì´ ë‚®ìŒì„ ì•Œë ¤ì£¼ë˜, ìœ ì‚¬í•œ ì‚¬ë¡€ë‚˜ ê´€ë ¨ ì¡°í•­ì„ ì†Œê°œí•˜ì„¸ìš”.
â€» ë§ˆì§€ë§‰ ì¤„ì— ì§ˆë¬¸ì´ ë²•ë¥ ì ìœ¼ë¡œ ì¶©ë¶„ížˆ ëª…í™•í•˜ë©´ `###yes`, ì•„ë‹ˆë©´ `###no`ë¥¼ ë¶™ì´ì„¸ìš”.
""",
            input_variables=[
                "chat_history",
                "user_query",
                "query_keywords",
                "faiss_keywords",
                "query_type",
                "legal_score",
            ],
        )

    async def generate(self, user_query: str):
        print("\nðŸ¤– [Legal AI]: ", end="", flush=True)

        # âœ… í‚¤ì›Œë“œ ì¶”ì¶œ
        query_keywords = faiss_kiwi.extract_keywords(user_query, top_k=5)
        faiss_keywords = faiss_kiwi.extract_top_keywords_faiss(
            user_query, self.faiss_db, top_k=5
        )
        legal_score = sum(1 for kw in query_keywords if kw in faiss_keywords) / max(
            len(query_keywords), 1
        )
        query_type = classify_legal_query(user_query, set(faiss_keywords))
        chat_history = self.memory.load_memory_variables({}).get("chat_history", "")

        # âœ… í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.prompt_template.format(
            chat_history=chat_history,
            user_query=user_query,
            query_keywords=", ".join(query_keywords),
            faiss_keywords=", ".join(faiss_keywords),
            legal_score=f"{legal_score:.2f}",
            query_type=query_type,
        )

        # âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        full_response = ""
        async for chunk in self.llm.astream(prompt):
            content = getattr(chunk, "content", str(chunk))
            if content:
                sys.stdout.write(content)
                sys.stdout.flush()
                full_response += content
        print("\n")

        self.memory.save_context(
            {"user_query": user_query}, {"response": full_response}
        )

        # âœ… ask_humanì„ í†µí•œ ###yes ì¹´ìš´íŠ¸ ê´€ë¦¬
        ask_result = await ask_human_for_information(
            user_query=user_query, llm1_answer=full_response, llm=self.llm
        )

        yes_count = ask_result.get("yes_count", 0)
        followup_question = ask_result.get("followup_question")

        escalate = False
        if yes_count >= 3 or check_user_wants_advanced_answer(user_query):
            escalate = True

        return {
            "initial_response": full_response,
            "escalate_to_advanced": escalate,
            "yes_count": yes_count,
            "query_type": query_type,
            "is_no": "###no" in full_response.lower(),
            "followup_question": followup_question,
        }
