import os
import sys
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from asyncio import Event

from app.chatbot.tool_agents.utils.utils import faiss_kiwi, classify_legal_query

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def load_llm():
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        temperature=0.6,
        max_tokens=512,
        streaming=True,
    )


class LegalChatbot:
    def __init__(self, faiss_db):
        self.llm = load_llm()
        self.memory = ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        )
        self.faiss_db = faiss_db
        self.prompt_template = PromptTemplate(
            template="""
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ í‚¤ì›Œë“œ ë° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë²•ë¥ ì  ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ğŸ’¬ ëŒ€í™” ê¸°ë¡:
{chat_history}

â“ ì‚¬ìš©ì ì§ˆë¬¸:
"{user_query}"

ğŸ§  ì‚¬ìš©ì ì…ë ¥ í‚¤ì›Œë“œ:
{query_keywords}

ğŸ“š FAISS ìœ ì‚¬ í‚¤ì›Œë“œ:
{faiss_keywords}

ğŸ“‚ ì§ˆë¬¸ ìœ í˜•: {query_type}
âš–ï¸ ë²•ë¥  ì—°ê´€ì„± ì ìˆ˜: {legal_score}

ğŸ“¢ ì§€ì‹œì‚¬í•­:
- ì§ˆë¬¸ ìœ í˜•ì´ **"legal"** ì´ë©´ â†’ ëª…í™•í•œ ë²•ë¥  ì¡°í•­ ë˜ëŠ” íŒë¡€ì— ê¸°ë°˜í•´ íŒë‹¨ì„ ë‚´ë ¤ì£¼ì„¸ìš”.
- ì§ˆë¬¸ ìœ í˜•ì´ **"nonlegal"** ì´ë©´ â†’ ë²•ì  ê´€ë ¨ì„±ì´ ë‚®ìŒì„ ì•Œë ¤ì£¼ë˜, ìœ ì‚¬í•œ ì‚¬ë¡€ë‚˜ ê´€ë ¨ ì¡°í•­ì„ ì†Œê°œí•˜ì„¸ìš”.
â€» ë§ˆì§€ë§‰ ì¤„ì— ì§ˆë¬¸ì´ ë²•ë¥ ì ìœ¼ë¡œ ì¶©ë¶„íˆ ëª…í™•í•˜ë©´ ###yes, ì•„ë‹ˆë©´ ###noë¥¼ ë¶™ì´ì„¸ìš”.
""",
            input_variables=[
                "chat_history",
                "user_query",
                "query_keywords",
                "faiss_keywords",
                "query_type",
                "legal_score",
            ],
        )

    async def generate(
        self,
        user_query: str,
        current_yes_count: int = 0,
        stop_event: Event = None,
    ):
        print("\nğŸ¤– [Legal AI]: ", end="", flush=True)

        query_keywords = faiss_kiwi.extract_keywords(user_query, top_k=5)
        faiss_keywords = faiss_kiwi.extract_top_keywords_faiss(
            user_query, self.faiss_db, top_k=5
        )
        legal_score = sum(1 for kw in query_keywords if kw in faiss_keywords) / max(
            len(query_keywords), 1
        )
        query_type = classify_legal_query(user_query, set(faiss_keywords))
        chat_history = self.memory.load_memory_variables({}).get("chat_history", "")

        prompt = self.prompt_template.format(
            chat_history=chat_history,
            user_query=user_query,
            query_keywords=", ".join(query_keywords),
            faiss_keywords=", ".join(faiss_keywords),
            legal_score=f"{legal_score:.2f}",
            query_type=query_type,
        )

        full_response = ""
        is_no_detected = False

        async for chunk in self.llm.astream(prompt):
            content = getattr(chunk, "content", str(chunk))
            if content:
                sys.stdout.write(content)
                sys.stdout.flush()
                full_response += content

                # ì‹¤ì‹œê°„ ê°ì§€
                if "###no" in full_response[-10:].lower():
                    is_no_detected = True
                    if stop_event:
                        stop_event.set()
                    break

        print("\n")

        self.memory.save_context(
            {"user_query": user_query}, {"response": full_response}
        )

        return {
            "initial_response": full_response,
            "escalate_to_advanced": False,
            "yes_count": current_yes_count,
            "query_type": query_type,
            "is_no": is_no_detected,
        }
