import os
import sys
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from asyncio import Event

from app.chatbot.tool_agents.utils.utils import faiss_kiwi, classify_legal_query

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def load_llm():
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        temperature=0.6,
        max_tokens=512,
        streaming=False,
    )


class LegalChatbot:
    def __init__(self, faiss_db):
        self.llm = load_llm()
        self.memory = ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        )
        self.faiss_db = faiss_db
        self.prompt_template = PromptTemplate(
            template="""
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ í‚¤ì›Œë“œ ë° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë²•ë¥ ì  ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ğŸ’¬ ëŒ€í™” ê¸°ë¡:
{chat_history}

â“ ì‚¬ìš©ì ì§ˆë¬¸:
"{user_query}"

ğŸ§  ì‚¬ìš©ì ì…ë ¥ í‚¤ì›Œë“œ:
{query_keywords}

ğŸ“š FAISS ìœ ì‚¬ í‚¤ì›Œë“œ:
{faiss_keywords}

ğŸ“‚ ì§ˆë¬¸ ìœ í˜•: {query_type}

ğŸ“¢ ì§€ì‹œì‚¬í•­:
- ì§ˆë¬¸ ìœ í˜•ì´ **"legal"** ì´ë©´ â†’ ëª…í™•í•œ ë²•ë¥  ì¡°í•­ ë˜ëŠ” íŒë¡€ì— ê¸°ë°˜í•˜ì—¬ íŒë‹¨ì„ ë‚´ë ¤ì£¼ì„¸ìš”.
- ì§ˆë¬¸ ìœ í˜•ì´ **"nonlegal"** ì´ë©´ â†’ ë²•ì  ê´€ë ¨ì„±ì´ ë‚®ìŒì„ ì•Œë¦¬ê³ , ìœ ì‚¬ ì‚¬ë¡€ë‚˜ ê´€ë ¨ ì¡°í•­ì„ ê°„ëµíˆ ì–¸ê¸‰í•˜ì„¸ìš”.
- ì•„ë˜ì˜ í‰ê°€ ê¸°ì¤€ì— ë”°ë¼, ì§ˆë¬¸ì˜ ëª…í™•ì„±, ë²•ë¥ ì  ê´€ë ¨ì„±, ê·¸ë¦¬ê³  í•„ìš”í•œ ì •ë³´ì˜ ì™„ì „ì„±ì„ ê°ê° 0(ë¶€ì¡±)ë¶€í„° 5(ë§¤ìš° ì¶©ì¡±)ê¹Œì§€ ì ìˆ˜ë¡œ í‰ê°€í•œ í›„,  
  í‰ê°€ í•­ëª©:
    1. ì§ˆë¬¸ì˜ ëª…í™•ì„±
    2. ë²•ë¥ ì  ê´€ë ¨ì„±
    3. í•„ìš”í•œ ì •ë³´ì˜ ì™„ì „ì„±
    
  ì´ì ì´ 10ì  ì´ìƒì´ë©´ ë§ˆì§€ë§‰ ì¤„ì— **"###yes"**ë¥¼, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ **"###no"**ë¥¼ ë°˜ë“œì‹œ ë¶™ì—¬ì„œ ì‘ë‹µí•´ ì£¼ì„¸ìš”.
""",
            input_variables=[
                "chat_history",
                "user_query",
                "query_keywords",
                "faiss_keywords",
                "query_type",
            ],
        )

    async def generate(
        self,
        user_query: str,
        current_yes_count: int = 0,
        stop_event: Event = None,
    ):
        # print("\nğŸ¤– [Legal AI]: ", end="", flush=True)

        query_keywords = faiss_kiwi.extract_keywords(user_query, top_k=5)
        faiss_keywords = faiss_kiwi.extract_top_keywords_faiss(
            user_query, self.faiss_db, top_k=5
        )
        legal_score = sum(1 for kw in query_keywords if kw in faiss_keywords) / max(
            len(query_keywords), 1
        )
        query_type = classify_legal_query(user_query, set(faiss_keywords))
        chat_history = self.memory.load_memory_variables({}).get("chat_history", "")

        prompt = self.prompt_template.format(
            chat_history=chat_history,
            user_query=user_query,
            query_keywords=", ".join(query_keywords),
            faiss_keywords=", ".join(faiss_keywords),
            legal_score=f"{legal_score:.2f}",
            query_type=query_type,
        )

        full_response = ""
        is_no_detected = False

        async for chunk in self.llm.astream(prompt):
            content = getattr(chunk, "content", str(chunk))
            if content:
                sys.stdout.write(content)
                sys.stdout.flush()
                full_response += content

                # ì‹¤ì‹œê°„ ê°ì§€
                if "###no" in full_response[-10:].lower():
                    is_no_detected = True
                    if stop_event:
                        stop_event.set()
                    break

        # print("\n")

        self.memory.save_context(
            {"user_query": user_query}, {"response": full_response}
        )

        return {
            "initial_response": full_response,
            "escalate_to_advanced": False,
            "yes_count": current_yes_count,
            "query_type": query_type,
            "is_no": is_no_detected,
        }
