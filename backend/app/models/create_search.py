import os
import sys
import torch
import asyncio
import numpy as np
from transformers import (
    BartForConditionalGeneration,
)
from sklearn.metrics.pairwise import cosine_similarity
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from safetensors.torch import load_file
from dotenv import load_dotenv
from kobart import get_pytorch_kobart_model, get_kobart_tokenizer
from kiwipiepy import Kiwi
from app.models.langchain_retriever import LangChainRetrieval
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
from app.models.tool_agents.tools import search_precedents
from app.models.tool_agents.tools import search_consultations

# âœ… Kiwi ê°ì²´ ì „ì—­ ìºì‹±
kiwi = Kiwi()
# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
# âœ… FAISS ë²¡í„°DB ë¡œë“œ
DB_FAISS_PATH = "./app/models/vectorstore/db_faiss"


def load_faiss():
    """FAISS ë¡œë“œ"""
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask"
        )
        return FAISS.load_local(
            DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True
        )
    except Exception as e:
        print(f"âŒ [FAISS ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None


def jaccard_similarity(set1, set2):
    """Jaccard ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ë¹„êµ"""
    intersection = set1.intersection(set2)
    union = set1.union(set2)
    return len(intersection) / len(union) if len(union) > 0 else 0


def extract_keywords(text, top_k=5):
    """í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•´ ëª…ì‚¬(NNG, NNP) ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    tokens = kiwi.tokenize(text)
    nouns = [token.form for token in tokens if token.tag in ("NNG", "NNP")]

    keyword_freq = {word: nouns.count(word) for word in set(nouns)}
    sorted_keywords = sorted(keyword_freq, key=keyword_freq.get, reverse=True)[:top_k]

    print(f"âœ… [í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼] {sorted_keywords}")
    return sorted_keywords


def filter_keywords_with_jaccard(user_keywords, faiss_keywords, threshold=0.15):
    """ìì¹´ë“œ ìœ ì‚¬ë„ë¥¼ í™œìš©í•˜ì—¬ FAISS í‚¤ì›Œë“œë¥¼ í•„í„°ë§ (ìœ ì € í‚¤ì›Œë“œ ìœ ì§€)"""
    filtered_keywords = set(user_keywords)  # âœ… ìœ ì € ì…ë ¥ í‚¤ì›Œë“œë¥¼ ë¬´ì¡°ê±´ í¬í•¨

    for faiss_word in faiss_keywords:
        max_sim = max(
            jaccard_similarity(set(faiss_word), set(user_word))
            for user_word in user_keywords
        )

        if max_sim >= threshold:  # âœ… ì¼ì • ìœ ì‚¬ë„ ì´ìƒì¸ FAISS í‚¤ì›Œë“œë§Œ ì¶”ê°€
            filtered_keywords.add(faiss_word)

    return list(filtered_keywords)  # âœ… ìµœì¢… í‚¤ì›Œë“œ ë°˜í™˜ (ìœ ì € í‚¤ì›Œë“œ í¬í•¨)


def adjust_faiss_keywords(user_input, faiss_keywords):
    """ìœ ì € ì…ë ¥ í‚¤ì›Œë“œì™€ FAISS í‚¤ì›Œë“œë¥¼ ëª¨ë‘ í¬í•¨í•˜ì—¬ ê²€ìƒ‰"""
    user_keywords = extract_keywords(user_input, top_k=5)  # âœ… ìœ ì € ì…ë ¥ í‚¤ì›Œë“œ ì¶”ì¶œ

    # âœ… ìœ ì € ì…ë ¥ í‚¤ì›Œë“œ + FAISS í‚¤ì›Œë“œ í†µí•© (ì¤‘ë³µ ì œê±°)
    adjusted_keywords = list(set(user_keywords + faiss_keywords))

    print(f"âœ… [ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ]: {adjusted_keywords}")
    return adjusted_keywords


def extract_top_keywords_faiss(user_input, faiss_db, top_k=8):
    """FAISS ê²€ìƒ‰ í›„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ (ìœ ì € ì…ë ¥ ë°˜ì˜)"""
    print(f"ğŸ” [FAISS í‚¤ì›Œë“œ ì¶”ì¶œ] ì…ë ¥: {user_input}")

    search_results = faiss_db.similarity_search(user_input, k=16)
    all_text = " ".join([doc.page_content for doc in search_results])

    faiss_keywords = extract_keywords(all_text, top_k)  # âœ… FAISSì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œ

    adjusted_keywords = adjust_faiss_keywords(
        user_input, faiss_keywords
    )  # âœ… ìœ ì € í‚¤ì›Œë“œ ë°˜ì˜
    print(f"âœ… [FAISS ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ] {adjusted_keywords}")
    return adjusted_keywords


def find_most_relevant_case(query, cases):
    """ê°€ì¥ ì—°ê´€ì„±ì´ ë†’ì€ íŒë¡€ ì„ íƒ"""
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask"
        )
        query_vector = embedding_model.embed_query(query)
        case_vectors = [embedding_model.embed_query(case["c_name"]) for case in cases]
        similarities = cosine_similarity([query_vector], case_vectors)[0]
        most_relevant_index = np.argmax(similarities)
        return cases[most_relevant_index]
    except Exception as e:
        print(f"âŒ [ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜] {e}")
        return cases[0]


langchain_retriever = LangChainRetrieval()

# âœ… BART ëª¨ë¸ ê²½ë¡œ
MODEL_PATH = "./app/models/model/1_bart/checkpoint-26606"

# âœ… ì „ì—­ ìºì‹±
bart_model = None
bart_tokenizer = None


def load_bart():
    """KoBART ëª¨ë¸ ë¡œë“œ (ì „ì—­ ìºì‹± ì ìš©)"""
    global bart_model, bart_tokenizer
    if bart_model is None or bart_tokenizer is None:
        try:
            print("ğŸ” KoBART ëª¨ë¸ ë¡œë“œ ì¤‘...")
            bart_model = BartForConditionalGeneration.from_pretrained(
                get_pytorch_kobart_model()
            )
            bart_tokenizer = get_kobart_tokenizer()
            bart_tokenizer.pad_token_id = bart_tokenizer.eos_token_id
            bart_tokenizer.model_max_length = 1024

            # âœ… `model.safetensors`ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
            state_dict = load_file(os.path.join(MODEL_PATH, "model.safetensors"))
            bart_model.load_state_dict(state_dict, strict=False)

            # âœ… ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì „í™˜
            bart_model.eval()
            print("âœ… KoBART ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ [KoBART ë¡œë“œ ì˜¤ë¥˜] {e}")
            bart_model, bart_tokenizer = None, None
    return bart_tokenizer, bart_model


def summarize_case(text, tokenizer, model):
    """íŒë¡€ ìš”ì•½: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ê¸¸ì–´ì•¼ ìš”ì•½ì„ ìˆ˜í–‰í•˜ë„ë¡ í•¨"""
    try:
        char_length = len(text)
        word_count = len(text.split())
        print(
            f"ğŸ” [DEBUG] ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ììˆ˜): {char_length}, ë‹¨ì–´ ìˆ˜: {word_count}"
        )

        if word_count < 5 or char_length < 20:
            return "ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì§§ì•„ ìš”ì•½ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # âœ… í† í°í™” í›„ ì¸ì½”ë”©ëœ ê°’ í™•ì¸ (clamp ì œê±° & padding ì¶”ê°€)
        input_ids = tokenizer.encode(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,  # âœ… íŒ¨ë”© ì¶”ê°€ë¡œ ì•ˆì •ì  í† í° ìƒì„±
        )
        print(f"ğŸ” [DEBUG] BART input_ids: {input_ids}")

        print("ğŸš€ [INFO] `generate()` ì‹¤í–‰ ì‹œì‘")

        summary_ids = model.generate(
            input_ids,
            max_length=200,  # âœ… ìµœëŒ€ ê¸¸ì´ ì¦ê°€ (150 â†’ 200)
            min_length=100,  # âœ… ìµœì†Œ ê¸¸ì´ ì¤„ì„ (149 â†’ 100)
            num_beams=3,  # âœ… beams ê°ì†Œë¡œ ì†ë„ ì¦ê°€ (5 â†’ 3)
            early_stopping=True,
            no_repeat_ngram_size=5,  # âœ… ë°˜ë³µ ë°©ì§€ (4 â†’ 5)
            repetition_penalty=2.5,  # âœ… ë°˜ë³µ ìµœì†Œí™” (2.2 â†’ 2.5)
            length_penalty=0.8,  # âœ… ê¸¸ì´ ì œí•œ ì™„í™” (1.0 â†’ 0.8)
        )

        print(f"ğŸ” [DEBUG] summary_ids: {summary_ids}")

        decoded = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        print(f"ğŸ” [DEBUG] ìš”ì•½ ê²°ê³¼: {decoded}")

        return decoded

    except Exception as e:
        print(f"âŒ [íŒë¡€ ìš”ì•½ ì˜¤ë¥˜] {e}")
        return "âŒ ìš”ì•½ ì‹¤íŒ¨"



@lru_cache(maxsize=1000)
def get_bart_model():
    return load_bart()

executor = ThreadPoolExecutor(max_workers=10)


async def async_search_precedent(keyword):
    """ë¹„ë™ê¸° SQL íŒë¡€ ê²€ìƒ‰ (ë©€í‹°ìŠ¤ë ˆë”© í™œìš©)"""
    loop = asyncio.get_running_loop()  # âœ… Python 3.10+ì—ì„œëŠ” get_running_loop() ì‚¬ìš©
    return await loop.run_in_executor(executor, search_precedents, keyword)


async def async_search_consultation(keyword):
    """ë¹„ë™ê¸° ë²•ë¥  ìƒë‹´ ê²€ìƒ‰ (ë©€í‹°ìŠ¤ë ˆë”© í™œìš©)"""
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(executor, search_consultations, keyword)


async def search(query: str):
    """FAISS + SQL + ë²•ë¥  ìƒë‹´ ê²€ìƒ‰ ìµœì í™” (ë¹„ë™ê¸° ì ìš©)"""
    print(f"\nğŸ” [INFO] ê²€ìƒ‰ ì‹¤í–‰: {query}")

    faiss_db = load_faiss()
    if not faiss_db:
        print("âŒ [ì˜¤ë¥˜] FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"error": "FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    # âœ… **FAISS í‚¤ì›Œë“œ ì¶”ì¶œ**
    keywords = extract_top_keywords_faiss(query, faiss_db, 5)
    print(f"âœ… [FAISS í‚¤ì›Œë“œ ì¶”ì¶œ] {keywords}")

    # âœ… **ê° í‚¤ì›Œë“œë³„ íŒë¡€ ê²€ìƒ‰ì„ ë³‘ë ¬ ì‹¤í–‰**
    search_tasks = [
        asyncio.create_task(async_search_precedent(keyword)) for keyword in keywords
    ]
    results_list = await asyncio.gather(*search_tasks)

    # ğŸ˜‰ ì•Œì•„ì„œ ì¡°ì •
    all_results = [item for sublist in results_list for item in sublist]
    unique_results = {r["pre_number"]: r for r in all_results}.values()
    results_sql = list(unique_results)[:2]
    print(f"âœ… [SQL ìµœì¢… ê²€ìƒ‰ ê²°ê³¼] ì´ {len(results_sql)}ê°œ íŒë¡€ ì„ íƒ")

    if not results_sql:
        print("âŒ [SQL ê²€ìƒ‰ ì‹¤íŒ¨] í•´ë‹¹ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {
            "search_result": "í•´ë‹¹ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "keywords_used": keywords,
            "consultation_result": "ë²•ë¥  ìƒë‹´ ê²€ìƒ‰ ì‹¤íŒ¨",
            "precedent_detail": "ì—†ìŒ",
            "summary": "BART ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ",
            "bert_prediction": "BERT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ",
            "final_answer": "ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        }

    # âœ… **ê°€ì¥ ì—°ê´€ì„± ë†’ì€ íŒë¡€ ì„ íƒ**
    most_relevant_precedent = find_most_relevant_case(query, results_sql)
    print(f"âœ… [ì„ íƒëœ íŒë¡€] {most_relevant_precedent['c_name']}")

    # âœ… **íŒë¡€ ìƒì„¸ ì •ë³´**
    precedent_detail = f"""
     ì‚¬ê±´ë²ˆí˜¸: {most_relevant_precedent["c_number"]}
     ì‚¬ê±´ì¢…ë¥˜: {most_relevant_precedent["c_type"]}
     íŒê²°ì¼: {most_relevant_precedent["j_date"]}
     ë²•ì›: {most_relevant_precedent["court"]}
     ë‚´ìš©ìš”ì•½: {most_relevant_precedent["c_name"]}
     ì›ë¬¸ ë§í¬: {most_relevant_precedent["d_link"]}
    """

    # âœ… **ë²•ë¥  ìƒë‹´ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ**
    consultation_keywords = extract_keywords(most_relevant_precedent["c_name"])

    # âœ… **ë²•ë¥  ìƒë‹´ ê²€ìƒ‰ì„ ë¹„ë™ê¸° ì‹¤í–‰ (ì™„ì „í•œ ë³‘ë ¬í™”)**
    consultation_tasks = [
        asyncio.create_task(async_search_consultation(keyword))
        for keyword in consultation_keywords
    ]
    consultation_results_list = await asyncio.gather(*consultation_tasks)

    # âœ… **ë²•ë¥  ìƒë‹´ ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°**
    all_consultations = [
        item for sublist in consultation_results_list for item in sublist
    ]
    unique_consultations = {r["id"]: r for r in all_consultations}.values()
    consultation_results = list(unique_consultations)[:5]  # âœ… ìµœëŒ€ 5ê°œ ì„ íƒ
    print(f"âœ… [ë²•ë¥  ìƒë‹´ ê²€ìƒ‰ ê²°ê³¼] ê°œìˆ˜: {len(consultation_results)}")

    # âœ… **ë²•ë¥  ìƒë‹´ ê²°ê³¼ë¥¼ `BART` ìš”ì•½ì— ì…ë ¥**
    consultation_text = "\n\n".join(
        [c["answer"] for c in consultation_results]  # âœ… ë‹µë³€ë§Œ ì‚¬ìš©
    )

    # âœ… **BART ìš”ì•½ ìˆ˜í–‰**
    summary = summarize_case(consultation_text, *get_bart_model())
    print(f"âœ… [BART ìš”ì•½ ì™„ë£Œ] {summary[:100]}...")

    # âœ… **LangChainì„ í™œìš©í•œ ìµœì¢… ë‹µë³€ ìƒì„±**
    final_answer = langchain_retriever.generate_legal_answer(query, summary)
    print(f"âœ… [LLM ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ] {final_answer[:100]}...")

    return {
        "search_result": results_sql,
        "keywords_used": keywords,
        "consultation_result": consultation_results,
        "precedent_detail": precedent_detail,
        "summary": summary,
        "final_answer": final_answer,
    }


def main():
    """CLI ê¸°ë°˜ ë²•ë¥  AI ì‹¤í–‰"""
    print("âœ… [ì‹œì‘] ë²•ë¥  AI ì‹¤í–‰")

    load_faiss()
    get_bart_model()

    while True:
        user_query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ")
        if user_query.lower() == "exit":
            break

        # âœ… **ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰**
        result = asyncio.run(search(user_query))

        print("\nğŸ“Œ [SQL ê²€ìƒ‰ ê²°ê³¼]:", result["search_result"])
        print("\nğŸ“Œ [ì‚¬ìš©ëœ í‚¤ì›Œë“œ]:", result["keywords_used"])
        print("\nğŸ“Œ [ë²•ë¥  ìƒë‹´ ê²€ìƒ‰ ê²°ê³¼]:", result["consultation_result"])
        print("\nğŸ“Œ [ì„ íƒëœ íŒë¡€ ìƒì„¸ ì •ë³´]:", result["precedent_detail"])
        print("\nğŸ“Œ [BART ìš”ì•½]:", result["summary"])
        print("\nğŸ¤– [LLM ìµœì¢… ë‹µë³€]:", result["final_answer"])


if __name__ == "__main__":
    main()
